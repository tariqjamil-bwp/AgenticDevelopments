{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If you haven't covered langmem and basic openai-agents-sdk go through these notebooks first:\n",
        "\n",
        "1. [Understand LangMem Core APIs](https://colab.research.google.com/drive/1YJNrnQRMgeNTigIuWOfykt-Z5L_DDmsa?usp=sharing)\n",
        "\n",
        "2. [A Basic Example of using Memory Tools with OpenAI Agents SDK](https://colab.research.google.com/drive/1xgSUeJPIBKyjpM868PsvmCZaCof-s2vB?usp=sharing)"
      ],
      "metadata": {
        "id": "5veeqGv2RTQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI Agents SDK with LangMem Memory Tools & Persistent Store**"
      ],
      "metadata": {
        "id": "o92NPvIPpmCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7teAe2rONlBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25cd5289-dd7b-4160-a6fa-28e9bbb12841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m836.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents langmem langchain-google-genai langmem_adapter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "z3_PWpn4OIyT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provider Config"
      ],
      "metadata": {
        "id": "ZTOibVJuN_pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "MODEL = \"gemini-2.0-flash\"\n",
        "\n"
      ],
      "metadata": {
        "id": "zvmgohqY0-Wm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "from agents import OpenAIChatCompletionsModel, set_tracing_disabled\n",
        "\n",
        "client = AsyncOpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=BASE_URL\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n",
        "\n",
        "set_tracing_disabled(disabled=False)"
      ],
      "metadata": {
        "id": "A2D6vm9aRy8i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persistent Store Setup"
      ],
      "metadata": {
        "id": "UPRt1Qi5rzmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q langgraph-checkpoint-postgres"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0SXApSbsxbh",
        "outputId": "559112b0-3387-4fb3-9379-5f3445570988"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m194.6/199.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Postgress DB with PGVector Extension Enabled.\n",
        "\n",
        "Neon already have PG Vector Extension installed. We just have to enable it and in our case it is enabled with the LangGraph Setup\n",
        "\n",
        "**TODO: Just signup at Cokroach DB or NEON and get a Postgres DB URL.**\n",
        "\n",
        "*Note: To check if the pgvector extension is enabled in your Neon Postgres database, you can run the \\dx command in psql or the Neon SQL Editor and look for an entry named \"vector\" in the extensions list. CREATE EXTENSION IF NOT EXISTS vector;.*"
      ],
      "metadata": {
        "id": "5jbkIJens0A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langgraph.store.postgres import AsyncPostgresStore\n",
        "\n",
        "# conn_string = \"postgresql://mr.test:testpass@host1.aws.neon.tech/op-ag-store?sslmode=require\"\n",
        "conn_string = userdata.get(\"PG_URL\")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n"
      ],
      "metadata": {
        "id": "s2XuvS5osnrV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Context Manager for PostGres"
      ],
      "metadata": {
        "id": "vrk44m86vnHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract the store creation into its own asynchronous context manager. This helps reduce duplication and makes the code more modular. For example, you can use Python's asynccontextmanager from the contextlib module to create a helper function:"
      ],
      "metadata": {
        "id": "5pYe2IOLzpJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import asynccontextmanager\n",
        "from langgraph.store.postgres import AsyncPostgresStore\n",
        "from langgraph.store.postgres.base import PoolConfig\n",
        "\n",
        "@asynccontextmanager\n",
        "async def get_store():\n",
        "    async with AsyncPostgresStore.from_conn_string(\n",
        "        conn_string,\n",
        "        index={\n",
        "            \"dims\": 768,\n",
        "            \"embed\": GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "        },\n",
        "        pool_config=PoolConfig(\n",
        "            min_size=5,\n",
        "            max_size=20\n",
        "        )\n",
        "    ) as store:\n",
        "        yield store\n"
      ],
      "metadata": {
        "id": "QvufXQaLzlSX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run Once to setup and run Migrations\n",
        "async with get_store() as store:\n",
        "    await store.setup()  # Run migrations. Done once\n"
      ],
      "metadata": {
        "id": "LIPAVSXASbFr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now check the Tables created in your DB."
      ],
      "metadata": {
        "id": "wl1-0a_-v5r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. OpenAI Agents SDK with Memory Layer"
      ],
      "metadata": {
        "id": "E_jESzWAsu91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "namespace=(\"assistant\", \"collection\")"
      ],
      "metadata": {
        "id": "uLS65BwEZ8zh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Memory Tools"
      ],
      "metadata": {
        "id": "JpVp79FMigAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_system_prompt_memory = \"\"\"\n",
        "< Role >\n",
        "You are Junaids executive assistant. You are a top-notch executive assistant who cares about AI Agents and performing as well as possible.\n",
        "</ Role >\n",
        "\n",
        "< Tools >\n",
        "You have access to the following tools to help manage Junaid's communications and schedule:\n",
        "\n",
        "1. manage_memory - Store any relevant information about contacts, actions, discussion, etc. in memory for future reference\n",
        "2. search_memory - Search for any relevant information that may have been stored in memory\n",
        "</ Tools >\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R_huA96aerqg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: We need to dig in langgraph store and then decide if we shall provide store once or like below."
      ],
      "metadata": {
        "id": "s5-FWpSDtaFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "namespace_template=('assistant', 'collection')"
      ],
      "metadata": {
        "id": "Tla2_yU7-oAU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a store provider for live DB connections (Dynamic Mode):\n",
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "from langmem_adapter import LangMemOpenAIAgentToolAdapter\n",
        "\n",
        "# Initialize the manage memory tool dynamically:\n",
        "manage_adapter = LangMemOpenAIAgentToolAdapter(\n",
        "    lambda store, namespace=None: create_manage_memory_tool(namespace=namespace, store=store),\n",
        "    store_provider=get_store,\n",
        "    namespace_template=namespace_template\n",
        ")\n",
        "manage_memory_tool = manage_adapter.as_tool()\n",
        "\n",
        "# Initialize the search memory tool dynamically:\n",
        "search_adapter = LangMemOpenAIAgentToolAdapter(\n",
        "    lambda store, namespace=None: create_search_memory_tool(namespace=namespace, store=store),\n",
        "    store_provider=get_store,\n",
        "    namespace_template=namespace_template\n",
        ")\n",
        "search_memory_tool = search_adapter.as_tool()\n"
      ],
      "metadata": {
        "id": "XTnIGsIWmps8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Note on Dynamic NameSpace. In LangMem Documentation we can see that we can have dynamic namespaces like (\"assistant\": {junaid}). With namespace_template and RunContextWrapper we can implement similar pattern here. More on this in next lesson."
      ],
      "metadata": {
        "id": "79-ttpTSxIIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools= [\n",
        "    manage_memory_tool,\n",
        "    search_memory_tool\n",
        "]\n"
      ],
      "metadata": {
        "id": "PnZLWjE9e4-w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVU0Z7N95Nth",
        "outputId": "826c345e-bdd1-4bcc-95b4-2bd86e12d99e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FunctionTool(name='manage_memory', description='Create, update, or delete persistent MEMORIES to persist across conversations.\\nInclude the MEMORY ID when updating or deleting a MEMORY. Omit when creating a new MEMORY - it will be created for you.\\nProactively call this tool when you:\\n\\n1. Identify a new USER preference.\\n2. Receive an explicit USER request to remember something or otherwise alter your behavior.\\n3. Are working and want to record important context.\\n4. Identify that an existing MEMORY is incorrect or outdated.', params_json_schema={'properties': {'content': {'default': '', 'title': 'Content', 'type': 'string'}, 'action': {'enum': ['create', 'update', 'delete'], 'title': 'Action', 'type': 'string'}, 'id': {'default': '', 'title': 'Id', 'type': 'string'}}, 'required': ['action'], 'title': 'manage_memoryArgs', 'type': 'object'}, on_invoke_tool=<bound method LangMemOpenAIAgentToolAdapter._on_invoke_tool of <langmem_adapter.openai_agents_sdk.LangMemOpenAIAgentToolAdapter object at 0x7d6ea4587390>>, strict_json_schema=True),\n",
              " FunctionTool(name='search_memory', description='Search your long-term memories for information relevant to your current context.', params_json_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}, 'limit': {'title': 'Limit', 'type': 'integer'}, 'offset': {'title': 'Offset', 'type': 'integer'}, 'filter': {'default': '', 'title': 'Filter', 'type': 'string'}}, 'required': ['query', 'limit', 'offset'], 'title': 'search_memoryArgs', 'type': 'object'}, on_invoke_tool=<bound method LangMemOpenAIAgentToolAdapter._on_invoke_tool of <langmem_adapter.openai_agents_sdk.LangMemOpenAIAgentToolAdapter object at 0x7d6e6b5abe50>>, strict_json_schema=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=agent_system_prompt_memory,\n",
        "    model=model,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "async def run_example(message: str):\n",
        "\n",
        "    result = await Runner.run(\n",
        "        agent,\n",
        "        message,\n",
        "    )\n",
        "    print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(run_example(\"So we are building Memoery Layer for AI Agent\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgOZPynGi_wT",
        "outputId": "5299be32-45b3-4f3d-bf3b-976ca6be1b84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's right! We're building a memory layer for AI Agents. This is a crucial component for enabling agents to learn and improve over time. How can I assist you further with this project?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Remember Ahmad is my Friend?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLk0mNHrwBlp",
        "outputId": "eae530ff-e64e-467e-aabd-0e2ad7b7875d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. I've saved that Ahmad is your friend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Who are my friends and what am I building\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9fPzNpBgsAO",
        "outputId": "e928dd87-9f4a-441b-a4ab-e0fadf6ce896"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. Based on my memory, Ahmad is your friend. I'm still searching for what you are building. Can you provide more context, or was that all you needed for now?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"It's not Ahmad who is my Friend instead Muhammad only?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhv5n33_oMpT",
        "outputId": "363d0c07-ae9b-4fb1-9a7c-e2561f4a5bbe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n",
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I have updated the memory to reflect that Muhammad is your friend, not Ahmad.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(run_example(\"Do you know who is my friend\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Ikxzx2xIbO",
        "outputId": "2da38d00-e044-453b-d232-d9603e3481d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I found a memory that says Muhammad is your friend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async with get_store() as store:\n",
        "  res = await store.asearch(namespace)\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5wf64VL0G-9",
        "outputId": "64b5da5f-63d0-4bad-9646-f41537a9dfc6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Item(namespace=['assistant', 'collection'], key='aa9d3d53-f428-484d-b2d0-0c1b5fded786', value={'content': \"Muhammad is Junaid's friend.\"}, created_at='2025-03-31T01:12:05.001795+00:00', updated_at='2025-03-31T01:12:43.388159+00:00', score=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic NameSpaces"
      ],
      "metadata": {
        "id": "km05LiD9xY6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class UserInfo(BaseModel):\n",
        "  username: str\n",
        "\n",
        "namespace_template=(\"assistant\", \"{username}\", \"collection\")"
      ],
      "metadata": {
        "id": "fLJIhYryxbS-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
        "from langmem_adapter import LangMemOpenAIAgentToolAdapter\n",
        "\n",
        "# Initialize the manage memory tool dynamically:\n",
        "manage_adapter = LangMemOpenAIAgentToolAdapter(\n",
        "    lambda store, namespace=None: create_manage_memory_tool(namespace=namespace, store=store),\n",
        "    store_provider=get_store,\n",
        "    namespace_template=namespace_template\n",
        ")\n",
        "manage_memory_tool = manage_adapter.as_tool()\n",
        "\n",
        "# Initialize the search memory tool dynamically:\n",
        "search_adapter = LangMemOpenAIAgentToolAdapter(\n",
        "    lambda store, namespace=None: create_search_memory_tool(namespace=namespace, store=store),\n",
        "    store_provider=get_store,\n",
        "    namespace_template=namespace_template\n",
        ")\n",
        "search_memory_tool = search_adapter.as_tool()"
      ],
      "metadata": {
        "id": "ucGEM4Ahxgoa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [manage_memory_tool, search_memory_tool]"
      ],
      "metadata": {
        "id": "THvxVnnbxxIH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_agent = Agent[UserInfo](\n",
        "    name=\"Response agent\",\n",
        "    instructions=agent_system_prompt_memory,\n",
        "    tools=tools,\n",
        "    model=model\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ZCXQguBExmpj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_result = await Runner.run(response_agent,\n",
        "                                   \"Remember We are building AI Agents to build Infra on Mars.\",\n",
        "                                   context=UserInfo(username=\"Junaid\")\n",
        "                                   )\n",
        "print(response_result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBhHL8q4xoFE",
        "outputId": "c8263e71-9fcd-4cf6-eff7-567effd640ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. I've stored that in memory.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_result = await Runner.run(response_agent,\n",
        "                                   \"What I told you about Mars\",\n",
        "                                   context=UserInfo(username=\"Junaid\")\n",
        "                                   )\n",
        "print(response_result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTJdxIkryDgw",
        "outputId": "9c2bb2f8-65dd-4d85-9ad7-092272d0ae24"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's what I remember you telling me about Mars: We are building AI Agents to build Infra on Mars.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------\n",
        "Now pass a different username"
      ],
      "metadata": {
        "id": "1GNBRXMVyNeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_result = await Runner.run(response_agent,\n",
        "                                   \"What I told you about Mars\",\n",
        "                                   context=UserInfo(username=\"Muhammad\")\n",
        "                                   )\n",
        "print(response_result.final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvhCMnJ3yMDL",
        "outputId": "313eb855-adcd-4374-e07f-298b31f81d3b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have any information about what you told me about Mars.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}