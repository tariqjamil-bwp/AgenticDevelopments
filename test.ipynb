{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "#from ollama import ChatOllama\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"llama3-70b-8192\", messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq()\n",
    "\n",
    "# Create a chat completion using a specific model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",  # Replace with the specific Groq model you're using\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the model's reply from the response\n",
    "model_reply = response.choices[0].message.content\n",
    "\n",
    "# Print the model's reply\n",
    "print(model_reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is an open-source framework that accelerates machine learning (ML) models by optimizing and compiling them for specific hardware targets. Here's how it works:\n",
      "\n",
      "**1. Model Optimization**: Groq takes in a trained ML model (e.g., TensorFlow, PyTorch) and applies various optimization techniques to reduce the model's computational complexity and memory footprint. This includes:\n",
      "\t* Quantization: reducing precision of model weights and activations to reduce compute and memory requirements.\n",
      "\t* Pruning: removing redundant or unnecessary neurons and connections to reduce model size and computation.\n",
      "\t* Knowledge distillation: transferring knowledge from a large model to a smaller, faster one.\n",
      "\n",
      "**2. Intermediate Representation (IR)**: Groq converts the optimized model into an intermediate representation (IR), which is a platform-agnostic, high-level representation of the model. This IR is used as input to the compilation step.\n",
      "\n",
      "**3. Compilation**: Groq compiles the IR into an optimized, hardware-specific representation of the model, tailored to the target device (e.g., GPUs, CPUs, TPUs, or FPGAs). This compilation step involves:\n",
      "\t* Low-level code generation: generating optimized assembly code or LLVM IR for the target device.\n",
      "\t* Memory allocation and management: optimizing memory access patterns and allocating memory resources efficiently.\n",
      "\t* Kernel fusion and scheduling: merging smaller compute kernels into larger, more efficient ones, and scheduling them for execution.\n",
      "\n",
      "**4. Runtime Acceleration**: Groq's runtime library provides the necessary infrastructure to execute the compiled model on the target device. This includes:\n",
      "\t* Device-specific runtime: managing device resources, memory allocation, and kernel execution.\n",
      "\t* Compute kernel execution: executing the optimized compute kernels on the target device.\n",
      "\n",
      "By applying these optimization and compilation techniques, Groq can significantly accelerate ML model inference on various hardware platforms, including edge devices, datacenter servers, and high-performance computing systems.\n",
      "\n",
      "Some of the benefits of using Groq include:\n",
      "\n",
      "* **Improving inference performance**: By optimizing and compiling models for specific hardware targets, Groq can achieve significant speedups in inference time.\n",
      "* **Reducing memory usage**: Groq's optimization techniques can reduce the memory footprint of ML models, making them more suitable for deployment on resource-constrained devices.\n",
      "* **Increasing energy efficiency**: By optimizing compute kernels and memory access patterns, Groq can help reduce the energy consumption of ML model inference.\n",
      "\n",
      "Overall, Groq provides a framework for accelerating machine learning models, enabling faster, more efficient, and more scalable deployment of ML models in various applications.\n"
     ]
    }
   ],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, api_key=None, model=\"llama3-70b-8192\"):\n",
    "        \"\"\"\n",
    "        Initialize the GroqAPI client with an API key and optional model name.\n",
    "        \"\"\"\n",
    "        from groq import Client  # Import inside class to avoid issues if not installed globally\n",
    "        import os\n",
    "        api_key = os.environ.get(\"GROQ_API_KEY\", api_key)\n",
    "        self.client = Client(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt):\n",
    "        \"\"\"\n",
    "        Send a prompt to the model and return the response.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            # Extract and return the assistant's reply\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = \"your_groq_api_key\"  # Replace with your actual Groq API key\n",
    "    groq = GroqLLM(api_key=api_key)\n",
    "\n",
    "    prompt = \"Explain how Groq accelerates machine learning models.\"\n",
    "    response = groq.chat(prompt)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ais_Model import GroqLLM\n",
    "llm = Groq(api_key='...', model_name='llama3-70b-8192')\n",
    "llm.invoke(chat_message={'user': 'Hello'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agents-4yRd6dJo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
