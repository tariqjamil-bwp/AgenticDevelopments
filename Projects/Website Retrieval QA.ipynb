{"cells":[{"cell_type":"markdown","metadata":{"id":"Z1hPnYfeou00"},"source":["https://medium.com/@datadrifters"]},{"cell_type":"markdown","metadata":{"id":"4eulO9WQou09"},"source":["install required libraries"]},{"cell_type":"code","source":["%pip install -q accelerate xformers\n","%pip install -q git+https://github.com/huggingface/transformers.git@main\n","%pip install -q optimum auto-gptq langchain"],"metadata":{"id":"4RWHPwQd1OLH","outputId":"427f4192-e3a2-455e-c5f8-406586d2f4fa","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import torch, transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM"],"metadata":{"id":"ZZyKmowV1VY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] == 8 else torch.float16\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"YWdqKDE31I3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MODEL_ID = 'tiiuae/falcon-7b-instruct'\n","#model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n","model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\""],"metadata":{"id":"G8jA0xEf1UQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading Model"],"metadata":{"id":"fUxYxfTk4ikN"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype = dtype,\n","    trust_remote_code=True\n","    )\n","\n","print(f\"Model device: {model.device}\")"],"metadata":{"id":"P0ZoZ5JI1ssS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_id,  padding_side=\"right\", truncation=True)\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"_YG4zBv53BNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Installing langchain & relevant packages"],"metadata":{"id":"1Sev-WCj4mL9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Hy2xUhRou1J"},"outputs":[],"source":["from transformers import AutoTokenizer, pipeline\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,\n","    temperature=0.7,\n","    top_p=0.95,\n","    repetition_penalty=1.15,\n","    #prompt_template\n",")\n","\n","from langchain.llms import HuggingFacePipeline\n","llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"code","execution_count":1,"metadata":{"vscode":{"languageId":"shellscript"},"id":"anWq2kYsou0-","executionInfo":{"status":"ok","timestamp":1693648658544,"user_tz":-300,"elapsed":43832,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["!pip3 install -qq langchain chromadb arxiv pymupdf\n","!pip3 install -qq sentence_transformers pypdf unstructured"]},{"cell_type":"markdown","metadata":{"id":"C8eioweFou1A"},"source":["# Importing required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UI_HpSAIou1B","executionInfo":{"status":"ok","timestamp":1693648684130,"user_tz":-300,"elapsed":25589,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"33c3c1a0-4f4a-4e16-d4af-f1757e241ce6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Error parsing requirements for transformers: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/transformers-4.32.1.dist-info/METADATA'\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# Imports\n","from chromadb.config import Settings\n","from urllib.error import HTTPError\n","from dataclasses import replace\n","#from dotenv import load_dotenv\n","from tqdm import tqdm\n","import numpy as np\n","#import tiktoken # OpenAI's open-source tokenizer\n","import chromadb\n","#import logging\n","import random # to sample multiple elements from a list\n","import arxiv\n","import time\n","import os # operating system dependent functionality, to walk through directories and files\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter # recursively tries to split by different characters to find one that works\n","from langchain.document_loaders import PyPDFDirectoryLoader # loads pdfs from a given directory\n","from langchain.chains import ConversationalRetrievalChain # looks up relevant documents from the retriever per history and question.\n","from langchain.text_splitter import CharacterTextSplitter # splits the content\n","from langchain.embeddings import HuggingFaceBgeEmbeddings # wrapper for HuggingFaceBgeEmbeddings models\n","from langchain import PromptTemplate, LLMChain\n","from langchain.document_loaders import ArxivLoader # loads paper for a given id from Arxiv\n","from langchain.document_loaders import PyPDFLoader # loads a given pdf\n","from langchain.document_loaders import DirectoryLoader\n","from langchain.document_loaders import TextLoader # loads a given text\n","from langchain.retrievers import ArxivRetriever # loads relevant papers for a given paper id from Arxiv\n","from chromadb.utils import embedding_functions # loads Chroma's embedding functions from OpenAI, HuggingFace, SentenceTransformer and others\n","from langchain.vectorstores import Chroma # wrapper around ChromaDB embeddings platform\n","from langchain.chains import RetrievalQA\n","from langchain.chains import RetrievalQAWithSourcesChain\n","from langchain import HuggingFaceHub # wrapper around HuggingFaceHub models\n","\n","!pip install -qq html2text\n","from langchain.document_loaders import WebBaseLoader\n","from langchain.document_transformers import Html2TextTransformer\n","from langchain.document_loaders import BSHTMLLoader\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"g8OeMnOJou1I"},"source":["creating the HuggingFacePipeline"]},{"cell_type":"markdown","metadata":{"id":"It9KgXVsou1G"},"source":["# Downloading HuggingFace BGE Embeddings"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vnR6NXTXou1G","executionInfo":{"status":"ok","timestamp":1693648767562,"user_tz":-300,"elapsed":16089,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["model_name = \"BAAI/bge-base-en\"\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","\n","embedding_function = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs={'device': 'cpu'},\n","    encode_kwargs=encode_kwargs\n",")"]},{"cell_type":"markdown","metadata":{"id":"XJyh6jlEou1H"},"source":["# Working with ChromaDB to store embeddings"]},{"cell_type":"code","source":["!pip install -qq requests beautifulsoup4 tqdm faiss-cpu"],"metadata":{"id":"GLZW7kW217I4","executionInfo":{"status":"ok","timestamp":1693648775772,"user_tz":-300,"elapsed":6953,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d12e75d-5e11-4e61-f3e9-4ef95ce118b7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["code 2"],"metadata":{"id":"67-i5M4YG5nC"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm\n","import json\n","from pathlib import Path\n","import pickle\n","import os\n","\n","main_web_url='https://docs.ray.io/en/latest/rllib/index.html'\n","full_url='https://www.neduet.edu.pk/'\n","domain = \"neduet.edu.pk\"\n","#url = 'https://docs.ray.io/en/latest/rllib/index.html'\n","\n","dicnry_links={}\n","#main_web_url='https://www.udst.edu.qa'\n","#url = 'https://www.udst.edu.qa'\n","dicnry_links[url]=0\n","counter=0\n","\n","destination_dir=\"web_data\"\n","!rm -rf 'web_data'\n","dest_f=main_web_url.split(\"://\")[-1]\n","print(dest_f)\n","destination_dir=destination_dir+'/'+dest_f.replace(\".\",\"_\")\n","print(destination_dir)\n","\n","if not os.path.isdir(destination_dir):\n","    os.makedirs(destination_dir, mode=511)\n","\n","# now go through the link and download pages as html or pdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkmIVCeVF-jS","executionInfo":{"status":"ok","timestamp":1693649022427,"user_tz":-300,"elapsed":385,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}},"outputId":"9896d1b2-1467-46d9-d18b-6024b650797b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["docs.ray.io/en/latest/rllib/index.html\n","web_data/docs_ray_io/en/latest/rllib/index_html\n"]}]},{"cell_type":"code","source":["from html import parser\n","import requests\n","import re\n","import urllib.request\n","from bs4 import BeautifulSoup\n","from collections import deque\n","from html.parser import HTMLParser\n","from urllib.parse import urlparse\n","import os\n","\n","# Regex pattern to match a URL\n","HTTP_URL_PATTERN = r'^http[s]*://.+'\n","\n","# Define root domain to crawl\n","#domain = \"openai.com\"\n","#full_url = \"https://openai.com/\"\n","\n","# Create a class to parse the HTML and get the hyperlinks\n","class HyperlinkParser(HTMLParser):\n","    def __init__(self):\n","        super().__init__()\n","        # Create a list to store the hyperlinks\n","        self.hyperlinks = []\n","\n","    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n","    def handle_starttag(self, tag, attrs):\n","        attrs = dict(attrs)\n","\n","        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n","        if tag == \"a\" and \"href\" in attrs:\n","            self.hyperlinks.append(attrs[\"href\"])\n","\n","# Function to get the hyperlinks from a URL\n","def get_hyperlinks(url):\n","\n","    # Try to open the URL and read the HTML\n","    try:\n","        # Open the URL and read the HTML\n","        with urllib.request.urlopen(url) as response:\n","\n","            # If the response is not HTML, return an empty list\n","            if not response.info().get('Content-Type').startswith(\"text/html\"):\n","                return []\n","\n","            # Decode the HTML\n","            html = response.read().decode('utf-8')\n","    except Exception as e:\n","        print(e)\n","        return []\n","\n","    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n","    parser = HyperlinkParser()\n","    parser.feed(html)\n","\n","    return parser.hyperlinks\n","\n","# Function to get the hyperlinks from a URL that are within the same domain\n","def get_domain_hyperlinks(local_domain, url):\n","    clean_links = []\n","    for link in set(get_hyperlinks(url)):\n","        clean_link = None\n","\n","        # If the link is a URL, check if it is within the same domain\n","        if re.search(HTTP_URL_PATTERN, link):\n","            # Parse the URL and check if the domain is the same\n","            url_obj = urlparse(link)\n","            if url_obj.netloc == local_domain:\n","                clean_link = link\n","\n","        # If the link is not a URL, check if it is a relative link\n","        else:\n","            if link.startswith(\"/\"):\n","                link = link[1:]\n","            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n","                continue\n","            clean_link = \"https://\" + local_domain + \"/\" + link\n","\n","        if clean_link is not None:\n","            if clean_link.endswith(\"/\"):\n","                clean_link = clean_link[:-1]\n","            clean_links.append(clean_link)\n","\n","    # Return the list of hyperlinks that are within the same domain\n","    return list(set(clean_links))\n","\n","\n","def crawl(url):\n","    # Parse the URL and get the domain\n","    local_domain = urlparse(url).netloc\n","\n","    # Create a queue to store the URLs to crawl\n","    queue = deque([url])\n","\n","    # Create a set to store the URLs that have already been seen (no duplicates)\n","    seen = set([url])\n","\n","    # Create a directory to store the text files\n","    if not os.path.exists(\"text/\"):\n","            os.mkdir(\"text/\")\n","\n","    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n","            os.mkdir(\"text/\" + local_domain + \"/\")\n","\n","    # Create a directory to store the csv files\n","    if not os.path.exists(\"processed\"):\n","            os.mkdir(\"processed\")\n","\n","    # While the queue is not empty, continue crawling\n","    while queue:\n","\n","        # Get the next URL from the queue\n","        url = queue.pop()\n","        print(url) # for debugging and to see the progress\n","\n","        # Save text from the url to a <url>.txt file\n","        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\") as f:\n","\n","            # Get the text from the URL using BeautifulSoup\n","            request = requests.get(url, verify=False)\n","            soup = BeautifulSoup(request, parser=\"html.parser\", verify_ssl=False)\n","\n","            # Get the text but remove the tags\n","            text = soup.get_text()\n","\n","            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n","            if (\"You need to enable JavaScript to run this app.\" in text):\n","                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n","\n","            # Otherwise, write the text to the file in the text directory\n","            f.write(text)\n","\n","        # Get the hyperlinks from the URL and add them to the queue\n","        for link in get_domain_hyperlinks(local_domain, url):\n","            if link not in seen:\n","                queue.append(link)\n","                seen.add(link)\n","\n","crawl(full_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"id":"N142Sg-IU6fx","executionInfo":{"status":"error","timestamp":1693649552529,"user_tz":-300,"elapsed":2417,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}},"outputId":"c5c1e01d-a3b7-46b1-9608-8784b21bf6fa"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["https://www.neduet.edu.pk/\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-bc57c8a81b6c>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-bc57c8a81b6c>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# Get the text from the URL using BeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_ssl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# Get the text but remove the tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# with the remaining **kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             if not original_builder and not (\n\u001b[1;32m    259\u001b[0m                     \u001b[0moriginal_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, empty_element_tags, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_NSMAPS_INVERTED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_namespace_prefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_NSMAPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLXMLTreeBuilderForXML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getNsTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TreeBuilder.__init__() got an unexpected keyword argument 'verify_ssl'"]}]},{"cell_type":"code","source":["#import urllib2\n","import urllib.request as urllib2\n","from urllib.parse import urljoin\n","\n","from bs4 import *\n","#from urlparse import urljoin\n","\n","def crawl(pages, depth=None):\n","    indexed_url = [] # a list for the main and sub-HTML websites in the main website\n","    for i in range(depth):\n","        for page in pages:\n","            if page not in indexed_url:\n","                indexed_url.append(page)\n","                try:\n","                    c = urllib2.urlopen(page)\n","                except:\n","                    print(\"Could not open %s\" % page)\n","                    continue\n","                soup = BeautifulSoup(c.read())\n","                data = [ele.text for ele in soup.find_all(text = True) if ele.text.strip() != '']\n","                print(data)\n","\n","                links = soup('a') #finding all the sub_links\n","                for link in links:\n","                    if 'href' in dict(link.attrs):\n","                        url = urljoin(page, link['href'])\n","                        if url.find(\"'\") != -1:\n","                            continue\n","                        url = url.split('#')[0]\n","                        if url[0:4] == 'http':\n","                            indexed_url.append(url)\n","        pages = indexed_url\n","    return indexed_url\n","\n","\n","pagelist=[url]\n","urls = crawl(pagelist, depth=2)\n","print(urls)\n","\n","for url in urls:\n","    try:\n","        print(url)\n","        if str(url).endswith(\".pdf\") or str(url).endswith(\".PDF\"):\n","            url=str(url).lower()\n","            file_name=url.split(\".pdf\")[-2]\n","            file_name=file_name.split(\"/\")[-1]\n","            file_name=file_name+\".pdf\"\n","            filename = Path(destination_dir+\"/\"+file_name)\n","            response = requests.get(url)\n","            filename.write_bytes(response.content)\n","\n","            print(\"pdf file name\",file_name)\n","            continue\n","\n","        response = requests.get(url)\n","        if not '.' in str(url):\n","            filename = url+'.html'\n","\n","        file_name=url\n","\n","        file_name=file_name.replace(\"/\",\"_\")\n","        print(file_name)\n","        with open(destination_dir+\"/\"+file_name,\"w\") as f:\n","            f.write(response.text)\n","    except:\n","        print(\"issue\")\n","        pass"],"metadata":{"id":"TvjDiFxJCp7T","executionInfo":{"status":"aborted","timestamp":1693648685409,"user_tz":-300,"elapsed":8,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","import os\n","\n","DATA_PATH=\"data/\"\n","DB_FAISS_PATH = \"vectorstores/db_faiss/\"\n","\n","links_path = \"/content/web_data/docs_ray_io/en/latest/rllib/index_html\"\n","\n","def create_vector_db(links_path):\n","    documents=[]\n","    processed_htmls=0\n","    processed_pdfs=0\n","    links_list = [l for l in os.listdir(links_path)]\n","\n","    for f in links_list: #os.listdir(links_path):#web_data\"):\n","        try:\n","            if f.endswith(\".pdf\"):\n","                pdf_path = os.path.join(links_path,f)#'./web_data/' + f\n","                loader = PyPDFLoader(pdf_path)\n","                documents.extend(loader.load())\n","                processed_pdfs+=1\n","            elif f.endswith(\".html\"):\n","                html_path = os.path.join(links_path,f)#'./web_data/' + f\n","                #loader = BSHTMLLoader(html_path)\n","                loader = WebBaseLoader(html_path)#, verify_ssl=False)\n","                loader.load()\n","                documents.extend(loader.load())\n","                processed_htmls+=1\n","        except:\n","            print(\"issue with \",f)\n","            pass\n","\n","    print(\"Processed\",processed_htmls,\"html files and \",processed_pdfs,\"pdf files\")\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","    texts=text_splitter.split_documents(documents)\n","\n","    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n","        model_kwargs={'device':'cpu'})\n","\n","    db=FAISS.from_documents(texts, embeddings)\n","    db.save_local(DB_FAISS_PATH)\n","    print(texts)\n","\n","if __name__==\"__main__\":\n","    create_vector_db(links_path)"],"metadata":{"id":"7AU_5tx17cUN","executionInfo":{"status":"aborted","timestamp":1693648685410,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain import PromptTemplate\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.llms import CTransformers\n","from langchain.chains import RetrievalQA\n","\n","DB_FAISS_PATH = \"vectorstores/db_faiss/\"\n","\n","custom_prompt_template='''Use the following pieces of information to answer the users question.\n","If you don't know the answer, please just say that you don't know the answer. Don't make up an answer.\n","\n","Context:{context}\n","question:{question}\n","\n","Only returns the helpful anser below and nothing else.\n","Helpful answer\n","'''\n","\n","def set_custom_prompt():\n","    '''\n","    Prompt template for QA retrieval for each vector store\n","    '''\n","    prompt =PromptTemplate(template=custom_prompt_template, input_variables=['context','question'])\n","    return prompt\n","\n","def retrieval_qa_chain(llm,prompt,db):\n","    qa_chain=RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=db.as_retriever(search_kwargs={'k':4}),\n","    return_source_documents=True,\n","    chain_type_kwargs={'prompt':prompt}\n","    )\n","    return qa_chain\n","\n","def qa_bot():\n","    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n","    model_kwargs={'device':'cpu'})\n","    db = FAISS.load_local(DB_FAISS_PATH,embeddings)\n","    qa_prompt=set_custom_prompt()\n","    qa = retrieval_qa_chain(llm,qa_prompt,db)\n","    return qa\n","\n","def final_result(query):\n","    qa_result=qa_bot()\n","    response=qa_result({'query':query})\n","    return response\n","\n","res=final_result('Hi')\n","answer=res[\"result\"]\n","sources=res[\"source_documents\"]\n","\n","if sources:\n","    answer+=f\"\\nSources: \"+str(str(sources))\n","else:\n","    answer+=f\"\\nNo Sources found\"\n","\n","print(answer)"],"metadata":{"id":"LhbHOeZaXd7g","executionInfo":{"status":"aborted","timestamp":1693648685410,"user_tz":-300,"elapsed":8,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res=final_result('I want to know more about the website which is been retrieved for qa')\n","answer=res[\"result\"]\n","sources=res[\"source_documents\"]\n","\n","if sources:\n","    answer+=f\"\\nSources: \"+str(str(sources))\n","else:\n","    answer+=f\"\\nNo Sources found\"\n","\n","print(answer)"],"metadata":{"id":"SEkF_E-4ax55","executionInfo":{"status":"aborted","timestamp":1693648685410,"user_tz":-300,"elapsed":8,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["web_links = [\n","    'https://docs.ray.io/'\n","    ]\n","\n","loader = WebBaseLoader(web_links, verify_ssl=False)\n","documents = loader.load()\n","\n","html2text = Html2TextTransformer()\n","texts = html2text.transform_documents(documents)\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","web_chunks = text_splitter.split_documents(texts)"],"metadata":{"id":"vFN2vVa2_5p5","executionInfo":{"status":"aborted","timestamp":1693648685411,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["web_chunks"],"metadata":{"id":"mTxJXIsl-Yu-","executionInfo":{"status":"aborted","timestamp":1693648685411,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiWIIvk8ou1H","executionInfo":{"status":"aborted","timestamp":1693648685411,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["persist_directory=\"./chromadb/\"\n","\n","vectordb = Chroma.from_documents(\n","    documents=web_chunks, # text data that you want to embed and store\n","    embedding=embedding_function, # used to convert the documents into embeddings\n","    persist_directory=persist_directory, # this tells Chroma where to store its data\n","    collection_name=\"RLIB_experiment\" #  gives a name to the collection of embeddings, which will be helpful for retrieving specific groups of embeddings later.\n",")\n","\n","vectordb.persist() # will make the database save any changes to the disk"]},{"cell_type":"markdown","metadata":{"id":"CaLS1JnBou1H"},"source":["# Retrieval QA with LangChain and Chroma"]},{"cell_type":"markdown","metadata":{"id":"dpLMR-xhou1I"},"source":["In case you run this code block second time after ChromaDB is created, you can use below line to create vectordb from ChromaDB. This will save time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33ablICrou1I","executionInfo":{"status":"aborted","timestamp":1693648685411,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"]},{"cell_type":"markdown","metadata":{"id":"64ZoCjoFou1J"},"source":["creating the QA chain with retriever to answer the questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWaocFGjou1K","executionInfo":{"status":"aborted","timestamp":1693648685412,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["RetrievalQA.from_chain_type.__doc__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gytJmdlsou1K","executionInfo":{"status":"aborted","timestamp":1693648685412,"user_tz":-300,"elapsed":9,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n","\n","retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n","                                  chain_type=\"stuff\",\n","                                  retriever=retriever,\n","                                  return_source_documents=True)"]},{"cell_type":"markdown","metadata":{"id":"ySQ00fmmou1L"},"source":["We are ready to ask questions!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-twlPTcxou1L","executionInfo":{"status":"aborted","timestamp":1693648685413,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["query = \"What is RLIB site all about?\"\n","llm_response = retrieval_qa_chain(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5BWkDltou1L","executionInfo":{"status":"aborted","timestamp":1693648685413,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["llm_response['result'].split('\\n')"]},{"cell_type":"markdown","metadata":{"id":"k2khhv5Pou1M"},"source":["You can also use retrieval QA chain with prompt templates, here's how you would do it for the same example as above:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mI8g5G4Lou1N","executionInfo":{"status":"aborted","timestamp":1693648685413,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["template = \"\"\"\n","{summaries}\n","{question}\n","\"\"\"\n","\n","retrieval_qa_chain_s = RetrievalQAWithSourcesChain.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever,\n","    return_source_documents=True,\n","    chain_type_kwargs={\n","        \"prompt\": PromptTemplate(\n","            template=template,\n","            input_variables=[\"summaries\", \"question\"],\n","        ),\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jer4k5p1ou1N","executionInfo":{"status":"aborted","timestamp":1693648685413,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["query = \"What are RLib Key Concepts?\"\n","llm_response = retrieval_qa_chain_s(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRE7WIv4ou1N","executionInfo":{"status":"aborted","timestamp":1693648685414,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["llm_response"]},{"cell_type":"markdown","metadata":{"id":"MdIOAUkjou1L"},"source":["We can also see the source and pages to generate the answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tL-oW6EHou1M","executionInfo":{"status":"aborted","timestamp":1693648685414,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["[source.metadata for source in llm_response[\"source_documents\"]]"]},{"cell_type":"markdown","source":["## *RLIB* Project"],"metadata":{"id":"7vON01Hj_6JD"}},{"cell_type":"code","source":["web_chunks"],"metadata":{"id":"_FpExZzhASOx","executionInfo":{"status":"aborted","timestamp":1693648685414,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install -q tj_preproc\n","#from tj_text.tj_preproc import text_preprocessing_ds, text_preprocessing"],"metadata":{"id":"1oDls6_oAowV","executionInfo":{"status":"aborted","timestamp":1693648685414,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading Documents from website"],"metadata":{"id":"f6VYGP2VHyHM"}},{"cell_type":"code","source":["import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"],"metadata":{"id":"VfzGykNAYVv9","executionInfo":{"status":"aborted","timestamp":1693648685414,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KtcbbgDcV53","executionInfo":{"status":"aborted","timestamp":1693648685415,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["!rm -rf pdfs\n","!wget -r -P pdfs -A pdf \"https://www.neduet.edu.pk/content/news\" --no-check-certificate -q"]},{"cell_type":"code","source":["!gdown --fuzzy \"https://drive.google.com/file/d/1dpYsXrgckTTSD0agwCe-FX6MIREF-wW3/view?usp=sharing\""],"metadata":{"id":"FVjq-FvpnoEE","executionInfo":{"status":"aborted","timestamp":1693648685415,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt install poppler-utils # required for image ocr\n","!pip install -q pytesseract # required for image ocr\n","!pip install -q pdf2image\n","!pip install -q pymupdf # required for fitz ocr\n","!pip install -q tesseract\n","\n","from IPython.display import HTML\n","import pdf2image\n","#import pytesseract#, tesseract\n","from pdf2image import convert_from_path\n","import tesseract"],"metadata":{"id":"FLUai_GAIXf1","executionInfo":{"status":"aborted","timestamp":1693648685415,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3m45kwnecoS","executionInfo":{"status":"aborted","timestamp":1693648685415,"user_tz":-300,"elapsed":10,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["import os\n","import shutil\n","from pathlib import Path\n","\n","def copy_pdf_files(source_folder: str, destination_folder: str):\n","    # Create the destination folder if it doesn't exist\n","    Path(destination_folder).mkdir(parents=True, exist_ok=True)\n","\n","    # Get a list of all PDF files in the source folder and its subfolders\n","    pdf_files = []\n","    for root, dirs, files in os.walk(source_folder):\n","        for file in files:\n","            if file.endswith(\".pdf\"):\n","                pdf_files.append(os.path.join(root, file))\n","\n","    # Copy the PDF files to the destination folder  (done for diagnostic purpose)\n","    for pdf_file in pdf_files:\n","        shutil.copy2(pdf_file, destination_folder)\n","\n","    # Sort the PDF files by creation date\n","    pdf_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n","    return pdf_files\n","\n","#!ls -all pdf_folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5HBYXf3hsxS","executionInfo":{"status":"aborted","timestamp":1693648685416,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["!rm -rf pdf_folder  # removing folder if exist\n","pdf_files = copy_pdf_files('pdfs', 'pdf_folder')   # copying files\n","pdf_files.append('/content/CV_Najeed_Updated_May 2023.pdf')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdYo3U7uGcte","executionInfo":{"status":"aborted","timestamp":1693648685416,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["pdf_files"]},{"cell_type":"markdown","source":["### Converting pdf to text (scanned pdf)"],"metadata":{"id":"nrRVTzxEdWhf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4Y-Df8c8NSF","executionInfo":{"status":"aborted","timestamp":1693648685416,"user_tz":-300,"elapsed":11,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["%%script true\n","!sudo apt install tesseract-ocr  # required for 1st algo\n","!pip install -q pdf2image\n","\n","import pdf2image\n","import pytesseract#, tesseract\n","from pdf2image import convert_from_path\n","import glob\n","'''\n","pdfs = glob.glob(r\"yourPath\\*.pdf\")\n","for pdf_path in pdfs:\n","    pages = convert_from_path(pdf_path, 500)\n","\n","    for pageNum,imgBlob in enumerate(pages):\n","        text = pytesseract.image_to_string(imgBlob, lang='eng')\n","\n","        with open(f'{pdf_path[:-4]}_page{pageNum}.txt', 'w') as the_file:\n","            the_file.write(text)\n","'''"]},{"cell_type":"markdown","source":["#### Reading text pdf file"],"metadata":{"id":"XdV_zPeRqkUk"}},{"cell_type":"code","source":["%%script true\n","from PyPDF2 import PdfReader\n","def process_pdf(file_path):\n","    pdf_reader = PdfReader(file_path)\n","    text = \"\"\n","    for page in pdf_reader.pages:\n","        text += page.extract_text()\n","    return text"],"metadata":{"id":"_xobDZC2qjn6","executionInfo":{"status":"aborted","timestamp":1693648685417,"user_tz":-300,"elapsed":72320,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Reading as scanned pdf"],"metadata":{"id":"kTFww_H6p3qU"}},{"cell_type":"code","source":["from pdf2image import convert_from_path\n","import pytesseract\n","\n","def process2_pdf(file_path):  #for scanned images\n","\n","    pages = convert_from_path(file_path, 500)\n","    text2=''\n","    for pageNum,imgBlob in enumerate(pages):\n","        text2 += pytesseract.image_to_string(imgBlob, lang='eng')\n","\n","    return text2"],"metadata":{"id":"BB1OvHYndtvG","executionInfo":{"status":"aborted","timestamp":1693648685418,"user_tz":-300,"elapsed":72319,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Reading as text pdf"],"metadata":{"id":"Bc8ZCVqcp_Gj"}},{"cell_type":"code","source":["import fitz\n","\n","def process3_pdf(file_path):\n","\n","    text3 = \"\"\n","    doc = fitz.open(file_path)\n","    for page in doc:\n","        text3 += page.get_text()\n","\n","    return text3"],"metadata":{"id":"kcvAWhofdvDh","executionInfo":{"status":"aborted","timestamp":1693648685418,"user_tz":-300,"elapsed":72317,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAF5fgn72inb","executionInfo":{"status":"aborted","timestamp":1693648685418,"user_tz":-300,"elapsed":72314,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["#process2_pdf(pdf_files[3])"]},{"cell_type":"code","source":["loader = DirectoryLoader('./pdf_folder/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n","pdfs = loader.load()"],"metadata":{"id":"SdMsVXxyLCBE","executionInfo":{"status":"aborted","timestamp":1693648685419,"user_tz":-300,"elapsed":72313,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(pdfs)"],"metadata":{"id":"ll2aJbbHrlx4","executionInfo":{"status":"aborted","timestamp":1693648685419,"user_tz":-300,"elapsed":72311,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 500,\n","    chunk_overlap  = 50\n",")\n","\n","pdfs_chunks = text_splitter.split_documents(pdfs)"],"metadata":{"id":"vvvYOu9bNPSd","executionInfo":{"status":"aborted","timestamp":1693648685419,"user_tz":-300,"elapsed":72309,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script true\n","chunks = []\n","for f in pdf_files:\n","    text = process3_pdf(f)\n","    if text =='': text = process2_pdf(f)\n","\n","    chunk = text_splitter.split_text(text)\n","    print(chunk)\n","    chunks.extend('\\n\\n')\n","    chunks.extend(chunk)\n","print(chunks)"],"metadata":{"id":"8y0mDcPwhmPT","executionInfo":{"status":"aborted","timestamp":1693648685420,"user_tz":-300,"elapsed":72308,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''doc_chunks = []\n","for chunk in [pdfs_chunks, web_chunks]:\n","    for doc in chunk:\n","        doc_chunks.append(doc)\n","'''\n","doc_chunks = pdfs_chunks"],"metadata":{"id":"PV_xyKccc2kn","executionInfo":{"status":"aborted","timestamp":1693648685420,"user_tz":-300,"elapsed":72306,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(web_chunks), len(pdfs_chunks), len(doc_chunks)"],"metadata":{"id":"pmzPL13fho-I","executionInfo":{"status":"aborted","timestamp":1693648685421,"user_tz":-300,"elapsed":72304,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for i in doc_chunks:\n","#    print(type(i))\n","    #print(i)"],"metadata":{"id":"O96Pen8zcJpV","executionInfo":{"status":"aborted","timestamp":1693648685421,"user_tz":-300,"elapsed":72302,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KPd2KK6OjlJ","executionInfo":{"status":"aborted","timestamp":1693648685421,"user_tz":-300,"elapsed":72300,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["!rm -rf chroma_db\n","persist_directory=\"./chromadb/\"\n","\n","vectordb = Chroma.from_documents(\n","    documents=doc_chunks, # text data that you want to embed and store\n","    #documents=paper_chunks, # text data that you want to embed and store\n","    embedding=embedding_function, # used to convert the documents into embeddings\n","    persist_directory=persist_directory, # this tells Chroma where to store its data\n","    collection_name=\"NED_news_and_events\" #  gives a name to the collection of embeddings, which will be helpful for retrieving specific groups of embeddings later.\n",")\n","\n","vectordb.persist() # will make the database save any changes to the disk"]},{"cell_type":"markdown","metadata":{"id":"H5h6NdhpOjlK"},"source":["# Retrieval QA with LangChain and Chroma"]},{"cell_type":"markdown","metadata":{"id":"EqtxxAnYOjlL"},"source":["In case you run this code block second time after ChromaDB is created, you can use below line to create vectordb from ChromaDB. This will save time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ujw6lh6NOjlM","executionInfo":{"status":"aborted","timestamp":1693648685421,"user_tz":-300,"elapsed":72297,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"]},{"cell_type":"markdown","metadata":{"id":"41_3DjjdOjlM"},"source":["creating the QA chain with retriever to answer the questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TYqS4--OjlN","executionInfo":{"status":"aborted","timestamp":1693648685422,"user_tz":-300,"elapsed":72295,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["RetrievalQA.from_chain_type.__doc__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4QBHEpUOjlP","executionInfo":{"status":"aborted","timestamp":1693648685422,"user_tz":-300,"elapsed":72292,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n","\n","retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n","                                  chain_type=\"stuff\",\n","                                  retriever=retriever,\n","                                  return_source_documents=True)"]},{"cell_type":"markdown","metadata":{"id":"9dx7AF1fOjlQ"},"source":["We are ready to ask questions!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGuFzFtQOjlR","executionInfo":{"status":"aborted","timestamp":1693648685422,"user_tz":-300,"elapsed":72289,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["query = \"Is there any notification from NED University saying holiday in 1, September 2023?\"\n","llm_response = retrieval_qa_chain(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mS3ITAlCOjlR","executionInfo":{"status":"aborted","timestamp":1693648685422,"user_tz":-300,"elapsed":72287,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["llm_response['result'].split('\\n')"]},{"cell_type":"markdown","metadata":{"id":"AiuSlyxgOjlS"},"source":["You can also use retrieval QA chain with prompt templates, here's how you would do it for the same example as above:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jheg1vgROjlT","executionInfo":{"status":"aborted","timestamp":1693648685423,"user_tz":-300,"elapsed":72285,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["template = \"\"\"\n","You are an AI Assistant who responsd queries at NED University helpdesk. You are polite, helpful and gives as much\n","infromation as you can. If you don't know anything, you politely say I am sorry please consult NED website or helpline.\n","{summaries}\n","Human:{question}\n","Assistant:\n","\"\"\"\n","\n","retrieval_qa_chain_s = RetrievalQAWithSourcesChain.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever,\n","    return_source_documents=True,\n","    chain_type_kwargs={\n","        \"prompt\": PromptTemplate(\n","            template=template,\n","            input_variables=[\"summaries\", \"question\"],\n","        ),\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxWCuOZGOjlT","executionInfo":{"status":"aborted","timestamp":1693648685423,"user_tz":-300,"elapsed":72283,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["query = \"Tell me about university in 4 lines?\"\n","llm_response = retrieval_qa_chain_s(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8PviyPYOjlU","executionInfo":{"status":"aborted","timestamp":1693648685423,"user_tz":-300,"elapsed":72280,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["llm_response['answer']"]},{"cell_type":"markdown","metadata":{"id":"YRd4xdLlOjlU"},"source":["We can also see the source and pages to generate the answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkPnO3sqOjlV","executionInfo":{"status":"aborted","timestamp":1693648685423,"user_tz":-300,"elapsed":72277,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"outputs":[],"source":["[source.metadata for source in llm_response[\"source_documents\"]]"]},{"cell_type":"code","source":["history=''\n","Q=[\n","    \"Q1) what are the requirements for admission on migration basis in NED university?\",\n","    \"Q2) Will there be a holiday for NED on Friday, 1st of September?\",\n","    \"Q3) What data should be checked for correction for the students who have passed NED’s Pre-Admission Entry Test?\",\n","    \"Q4) List the academic programs offered by NED for undergraduate students?\",\n","    \"Q5) Provide details regarding undergraduate admissions in NED for the 2023 academic year.\",\n","    \"Q6) List the minimum requirements to apply for admission in NED.\",\n","    \"Q7) What are the requirements of admission in the category of differently able candidate?\",\n","    \"Q8) Outline the fee structure for undergraduate and postgraduate degrees in NED.\",\n","    \"Q9) What is the process of admission for foreign students in NED?\",\n","    \"Q10) Who is the current vice chancellor and pro vice chancellor of NED University?\"\n","    ]"],"metadata":{"id":"zKF-BAWklriA","executionInfo":{"status":"aborted","timestamp":1693648685424,"user_tz":-300,"elapsed":72276,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for q in Q:\n","    history=''\n","    llm_response = retrieval_qa_chain(q)\n","    a = llm_response['result']\n","    display(q)\n","    display(\"A:\",a)\n","    print('\\n')"],"metadata":{"id":"E9XmV4F4sMHn","executionInfo":{"status":"aborted","timestamp":1693648685424,"user_tz":-300,"elapsed":72274,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iDQrptqGu6hZ","executionInfo":{"status":"aborted","timestamp":1693648685424,"user_tz":-300,"elapsed":72272,"user":{"displayName":"Tariq Jamil","userId":"03028686656397088965"}}},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"19D7I9apIBk5cgFK35NSGDBLHmuaI0FGB","timestamp":1693547603549}]}},"nbformat":4,"nbformat_minor":0}