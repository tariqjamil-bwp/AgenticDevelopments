{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mIKYRTMtkJN"
      },
      "source": [
        "# Using RetrieveChat for Retrieve Augmented Code Generation and Question Answering\n",
        "\n",
        "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
        "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
        "\n",
        "RetrieveChat is a conversational system for retrieval-augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `AssistantAgent` and `RetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)). Essentially, `RetrieveUserProxyAgent` implement a different auto-reply mechanism corresponding to the RetrieveChat prompts.\n",
        "\n",
        "## Table of Contents\n",
        "We'll demonstrate six examples of using RetrieveChat for code generation and question answering:\n",
        "\n",
        "- [Example 1: Generate code based off docstrings w/o human feedback](#example-1)\n",
        "- [Example 2: Answer a question based off docstrings w/o human feedback](#example-2)\n",
        "- [Example 3: Generate code based off docstrings w/ human feedback](#example-3)\n",
        "- [Example 4: Answer a question based off docstrings w/ human feedback](#example-4)\n",
        "- [Example 5: Solve comprehensive QA problems with RetrieveChat's unique feature `Update Context`](#example-5)\n",
        "- [Example 6: Solve comprehensive QA problems with customized prompt and few-shot learning](#example-6)\n",
        "\n",
        "\n",
        "````{=mdx}\n",
        ":::info Requirements\n",
        "Some extra dependencies are needed for this notebook, which can be installed via pip:\n",
        "\n",
        "```bash\n",
        "pip install pyautogen[retrievechat] flaml[automl]\n",
        "```\n",
        "\n",
        "For more information, please refer to the [installation guide](/docs/installation/).\n",
        ":::\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUiwVF53tkJR"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "config_list = [{\"api_type\":\"groq\",\"model\":\"llama3-70b-8192\",\"api_key\":os.environ.get(\"GROQ_API_KEY\"),}]\n",
        "llm_config={\"config_list\" : config_list}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "glpiE_UTtkJR",
        "outputId": "7ff31a3a-ad42-4851-fc9c-59a8dd972364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models to use:  ['llama3-70b-8192']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import chromadb\n",
        "\n",
        "import autogen\n",
        "from autogen import AssistantAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "\n",
        "# Accepted file formats for that can be stored in\n",
        "# a vector database instance\n",
        "from autogen.retrieve_utils import TEXT_FORMATS\n",
        "\n",
        "#config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n",
        "\n",
        "assert len(config_list) > 0\n",
        "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBlDP-uqtkJT"
      },
      "source": [
        "````{=mdx}\n",
        ":::tip\n",
        "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
        ":::\n",
        "````\n",
        "\n",
        "## Construct agents for RetrieveChat\n",
        "\n",
        "We start by initializing the `AssistantAgent` and `RetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for AssistantAgent. The detailed instructions are given in the user message. Later we will use the `RetrieveUserProxyAgent.message_generator` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Arx9fv3MtkJU",
        "outputId": "217a5a6d-1088-42d3-953b-0fe7c0cc588b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted file formats for `docs_path`:\n",
            "['rtf', 'yml', 'epub', 'xml', 'pptx', 'ppt', 'jsonl', 'rst', 'csv', 'doc', 'htm', 'txt', 'docx', 'msg', 'yaml', 'pdf', 'json', 'md', 'html', 'odt', 'org', 'log', 'tsv', 'xlsx']\n"
          ]
        }
      ],
      "source": [
        "print(\"Accepted file formats for `docs_path`:\")\n",
        "print(TEXT_FORMATS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xlbf9DoutkJU",
        "outputId": "a545ebda-bdf7-4550-d587-8639f91edec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tjamil/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 1. create an AssistantAgent instance named \"assistant\"\n",
        "assistant = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    llm_config={\n",
        "        \"timeout\": 600,\n",
        "        \"cache_seed\": 42,\n",
        "        \"config_list\": config_list,\n",
        "    },\n",
        ")\n",
        "\n",
        "# 2. create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
        "# Refer to https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent\n",
        "# and https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/vectordb/chromadb\n",
        "# for more information on the RetrieveUserProxyAgent and ChromaVectorDB\n",
        "ragproxyagent = RetrieveUserProxyAgent(\n",
        "    name=\"ragproxyagent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    retrieve_config={\n",
        "        \"task\": \"code\",\n",
        "        \"docs_path\": [\n",
        "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
        "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
        "        ],\n",
        "        \"chunk_token_size\": 2000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"vector_db\": \"chroma\",\n",
        "        \"overwrite\": False,  # set to True if you want to overwrite an existing collection\n",
        "        \"get_or_create\": True,  # set to False if don't want to reuse an existing collection\n",
        "    },\n",
        "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "#import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#from autogen import AssistantAgent, RetrieveUserProxyAgent\n",
        "\n",
        "# Load the embedding model (you can replace this with any embedding model of your choice)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 1. Create an AssistantAgent instance named \"assistant\"\n",
        "assistant = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    llm_config={\n",
        "        \"timeout\": 600,\n",
        "        \"cache_seed\": 42,\n",
        "        \"config_list\": config_list,\n",
        "    },\n",
        ")\n",
        "\n",
        "# 2. Define a FAISS-based retrieval mechanism\n",
        "class FAISSVectorDB:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.index_path = index_path\n",
        "        self.dimension = 384  # Embedding dimension size for MiniLM\n",
        "        self.index = None\n",
        "\n",
        "        if os.path.exists(index_path):\n",
        "            self.index = faiss.read_index(index_path)\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatL2(self.dimension)  # Create new FAISS index\n",
        "\n",
        "    def add_to_index(self, docs):\n",
        "        # Convert documents to embeddings\n",
        "        embeddings = embedding_model.encode(docs)\n",
        "        # Add embeddings to the FAISS index\n",
        "        self.index.add(np.array(embeddings))\n",
        "        # Save the index to disk\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        # Convert query to embeddings\n",
        "        query_embedding = embedding_model.encode([query])\n",
        "        # Search in the FAISS index\n",
        "        distances, indices = self.index.search(np.array(query_embedding), top_k)\n",
        "        return distances, indices\n",
        "\n",
        "\n",
        "# 3. Initialize FAISSVectorDB\n",
        "faiss_db = FAISSVectorDB()\n",
        "\n",
        "# 4. Create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
        "ragproxyagent = RetrieveUserProxyAgent(\n",
        "    name=\"ragproxyagent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    retrieve_config={\n",
        "        \"task\": \"code\",\n",
        "        \"docs_path\": [\n",
        "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
        "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
        "        ],\n",
        "        \"chunk_token_size\": 2000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"vector_db\": faiss_db,  # Replace \"chroma\" with your FAISSVectorDB instance\n",
        "        \"overwrite\": False,\n",
        "        \"get_or_create\": True,\n",
        "    },\n",
        "    code_execution_config=False,  # Set to False if you don't want to execute the code\n",
        ")\n",
        "\n",
        "# Example search using FAISS\n",
        "query = \"Explain Spark integration\"\n",
        "distances, indices = faiss_db.search(query)\n",
        "print(f\"Search results: {indices}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrG7_WQCtkJV"
      },
      "source": [
        "### Example 1\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to help generate sample code and automatically run the code and fix errors if there is any.\n",
        "\n",
        "Problem: Which API should I use if I want to use FLAML for a classification task and I want to train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m04F7DNFtkJV",
        "outputId": "8ac134ad-83bf-4ef6-a9d2-e324a8c689d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 16:24:30,963 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - \u001b[32mUse the existing collection `autogen-docs`.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying to create collection.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-25 16:24:32,422 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 2 chunks.\u001b[0m\n",
            "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n",
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorDB returns doc_ids:  [['bdfbc921']]\n",
            "\u001b[32mAdding content of doc bdfbc921 to context.\u001b[0m\n",
            "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
            "\n",
            "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
            "context provided by the user.\n",
            "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
            "For code generation, you must obey the following rules:\n",
            "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
            "Rule 2. You must follow the formats below to write your code:\n",
            "```language\n",
            "# your code\n",
            "```\n",
            "\n",
            "User's question is: How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\n",
            "\n",
            "Context is: # Integrate - Spark\n",
            "\n",
            "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
            "\n",
            "- Use Spark ML estimators for AutoML.\n",
            "- Use Spark to run training in parallel spark jobs.\n",
            "\n",
            "## Spark ML Estimators\n",
            "\n",
            "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
            "\n",
            "### Data\n",
            "\n",
            "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
            "\n",
            "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
            "\n",
            "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
            "\n",
            "- `index_col` is the column name to use as the index, default is None.\n",
            "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
            "\n",
            "Here is an example code snippet for Spark Data:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "\n",
            "# Creating a dictionary\n",
            "data = {\n",
            "    \"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
            "    \"Age_Years\": [20, 15, 10, 7, 25],\n",
            "    \"Price\": [100000, 200000, 300000, 240000, 120000],\n",
            "}\n",
            "\n",
            "# Creating a pandas DataFrame\n",
            "dataframe = pd.DataFrame(data)\n",
            "label = \"Price\"\n",
            "\n",
            "# Convert to pandas-on-spark dataframe\n",
            "psdf = to_pandas_on_spark(dataframe)\n",
            "```\n",
            "\n",
            "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
            "\n",
            "Here is an example of how to use it:\n",
            "\n",
            "```python\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "columns = psdf.columns\n",
            "feature_cols = [col for col in columns if col != label]\n",
            "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "```\n",
            "\n",
            "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
            "\n",
            "### Estimators\n",
            "\n",
            "#### Model List\n",
            "\n",
            "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
            "\n",
            "#### Usage\n",
            "\n",
            "First, prepare your data in the required format as described in the previous section.\n",
            "\n",
            "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
            "\n",
            "Here is an example code snippet using SparkML models in AutoML:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "# prepare your data in pandas-on-spark format as we previously mentioned\n",
            "\n",
            "automl = flaml.AutoML()\n",
            "settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
            "    \"task\": \"regression\",\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=psdf,\n",
            "    label=label,\n",
            "    **settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
            "\n",
            "## Parallel Spark Jobs\n",
            "\n",
            "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
            "\n",
            "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
            "\n",
            "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
            "\n",
            "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
            "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
            "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
            "\n",
            "An example code snippet for using parallel Spark jobs:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "automl_experiment = flaml.AutoML()\n",
            "automl_settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"task\": \"regression\",\n",
            "    \"n_concurrent_trials\": 2,\n",
            "    \"use_spark\": True,\n",
            "    \"force_cancel\": True,  # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=dataframe,\n",
            "    label=label,\n",
            "    **automl_settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
            "\n",
            "```\n",
            "import pandas as pd\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "import flaml\n",
            "\n",
            "# assuming your data is in a pandas DataFrame, dataframe\n",
            "psdf = to_pandas_on_spark(dataframe)\n",
            "\n",
            "label = \"your_labelColumn\"\n",
            "columns = psdf.columns\n",
            "feature_cols = [col for col in columns if col != label]\n",
            "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "\n",
            "automl = flaml.AutoML()\n",
            "settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],\n",
            "    \"task\": \"classification\",\n",
            "    \"use_spark\": True,\n",
            "    \"force_cancel\": True,\n",
            "    \"n_concurrent_trials\": 2\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=psdf,\n",
            "    label=label,\n",
            "    **settings,\n",
            ")\n",
            "```\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "assistant.reset()\n",
        "\n",
        "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
        "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
        "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
        "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
        "code_problem = \"How can I use FLAML to perform a classification task and use spark to do parallel training. Train 30 seconds and force cancel jobs if time limit is reached.\"\n",
        "chat_result = ragproxyagent.initiate_chat(\n",
        "    assistant, message=ragproxyagent.message_generator, problem=code_problem, search_string=\"spark\"\n",
        ")  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain \"spark\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFNX_mXtkJW"
      },
      "source": [
        "### Example 2\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to answer a question that is not related to code generation.\n",
        "\n",
        "Problem: Who is the author of FLAML?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PFmyQjI9tkJW",
        "outputId": "c92559fd-3c13-4121-fa70-6a61f9445182"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n",
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorDB returns doc_ids:  [['7968cf3c', 'bdfbc921']]\n",
            "\u001b[32mAdding content of doc 7968cf3c to context.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mAdding content of doc bdfbc921 to context.\u001b[0m\n",
            "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
            "\n",
            "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
            "context provided by the user.\n",
            "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
            "For code generation, you must obey the following rules:\n",
            "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
            "Rule 2. You must follow the formats below to write your code:\n",
            "```language\n",
            "# your code\n",
            "```\n",
            "\n",
            "User's question is: Who is the author of FLAML?\n",
            "\n",
            "Context is: # Research\n",
            "\n",
            "For technical details, please check our research publications.\n",
            "\n",
            "- [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021flaml,\n",
            "    title={FLAML: A Fast and Lightweight AutoML Library},\n",
            "    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},\n",
            "    year={2021},\n",
            "    booktitle={MLSys},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021cfo,\n",
            "    title={Frugal Optimization for Cost-related Hyperparameters},\n",
            "    author={Qingyun Wu and Chi Wang and Silu Huang},\n",
            "    year={2021},\n",
            "    booktitle={AAAI},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021blendsearch,\n",
            "    title={Economical Hyperparameter Optimization With Blended Search Strategy},\n",
            "    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},\n",
            "    year={2021},\n",
            "    booktitle={ICLR},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{liuwang2021hpolm,\n",
            "    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},\n",
            "    author={Susan Xueqing Liu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ACL},\n",
            "}\n",
            "```\n",
            "\n",
            "- [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021chacha,\n",
            "    title={ChaCha for Online AutoML},\n",
            "    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},\n",
            "    year={2021},\n",
            "    booktitle={ICML},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wuwang2021fairautoml,\n",
            "    title={Fair AutoML},\n",
            "    author={Qingyun Wu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ArXiv preprint arXiv:2111.06495},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{kayaliwang2022default,\n",
            "    title={Mining Robust Default Configurations for Resource-constrained AutoML},\n",
            "    author={Moe Kayali and Chi Wang},\n",
            "    year={2022},\n",
            "    booktitle={ArXiv preprint arXiv:2202.09927},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{zhang2023targeted,\n",
            "    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},\n",
            "    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},\n",
            "    booktitle={International Conference on Learning Representations},\n",
            "    year={2023},\n",
            "    url={https://openreview.net/forum?id=0Ij9_q567Ma},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2023EcoOptiGen,\n",
            "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
            "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2303.04673},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2023empirical,\n",
            "    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n",
            "    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2306.01337},\n",
            "}\n",
            "```\n",
            "# Integrate - Spark\n",
            "\n",
            "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
            "\n",
            "- Use Spark ML estimators for AutoML.\n",
            "- Use Spark to run training in parallel spark jobs.\n",
            "\n",
            "## Spark ML Estimators\n",
            "\n",
            "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
            "\n",
            "### Data\n",
            "\n",
            "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
            "\n",
            "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
            "\n",
            "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
            "\n",
            "- `index_col` is the column name to use as the index, default is None.\n",
            "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
            "\n",
            "Here is an example code snippet for Spark Data:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "\n",
            "# Creating a dictionary\n",
            "data = {\n",
            "    \"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
            "    \"Age_Years\": [20, 15, 10, 7, 25],\n",
            "    \"Price\": [100000, 200000, 300000, 240000, 120000],\n",
            "}\n",
            "\n",
            "# Creating a pandas DataFrame\n",
            "dataframe = pd.DataFrame(data)\n",
            "label = \"Price\"\n",
            "\n",
            "# Convert to pandas-on-spark dataframe\n",
            "psdf = to_pandas_on_spark(dataframe)\n",
            "```\n",
            "\n",
            "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
            "\n",
            "Here is an example of how to use it:\n",
            "\n",
            "```python\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "columns = psdf.columns\n",
            "feature_cols = [col for col in columns if col != label]\n",
            "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "```\n",
            "\n",
            "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
            "\n",
            "### Estimators\n",
            "\n",
            "#### Model List\n",
            "\n",
            "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
            "\n",
            "#### Usage\n",
            "\n",
            "First, prepare your data in the required format as described in the previous section.\n",
            "\n",
            "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
            "\n",
            "Here is an example code snippet using SparkML models in AutoML:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "# prepare your data in pandas-on-spark format as we previously mentioned\n",
            "\n",
            "automl = flaml.AutoML()\n",
            "settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
            "    \"task\": \"regression\",\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=psdf,\n",
            "    label=label,\n",
            "    **settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
            "\n",
            "## Parallel Spark Jobs\n",
            "\n",
            "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
            "\n",
            "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
            "\n",
            "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
            "\n",
            "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
            "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
            "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
            "\n",
            "An example code snippet for using parallel Spark jobs:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "automl_experiment = flaml.AutoML()\n",
            "automl_settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"task\": \"regression\",\n",
            "    \"n_concurrent_trials\": 2,\n",
            "    \"use_spark\": True,\n",
            "    \"force_cancel\": True,  # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=dataframe,\n",
            "    label=label,\n",
            "    **automl_settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
            "\n",
            "The authors of FLAML are Chi Wang and Qingyun Wu.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "assistant.reset()\n",
        "\n",
        "qa_problem = \"Who is the author of FLAML?\"\n",
        "chat_result = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=qa_problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqmD9t-RtkJW"
      },
      "source": [
        "### Example 3\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to help generate sample code and ask for human-in-loop feedbacks.\n",
        "\n",
        "Problem: how to build a time series forecasting model for stock price using FLAML?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K4_hCrtbtkJW",
        "outputId": "afddda37-b9c6-4876-becb-d37f88d5c41e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n",
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorDB returns doc_ids:  [['bdfbc921', '7968cf3c']]\n",
            "\u001b[32mAdding content of doc bdfbc921 to context.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mAdding content of doc 7968cf3c to context.\u001b[0m\n",
            "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
            "\n",
            "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
            "context provided by the user.\n",
            "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
            "For code generation, you must obey the following rules:\n",
            "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
            "Rule 2. You must follow the formats below to write your code:\n",
            "```language\n",
            "# your code\n",
            "```\n",
            "\n",
            "User's question is: how to build a time series forecasting model for stock price using FLAML?\n",
            "\n",
            "Context is: # Integrate - Spark\n",
            "\n",
            "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
            "\n",
            "- Use Spark ML estimators for AutoML.\n",
            "- Use Spark to run training in parallel spark jobs.\n",
            "\n",
            "## Spark ML Estimators\n",
            "\n",
            "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
            "\n",
            "### Data\n",
            "\n",
            "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
            "\n",
            "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
            "\n",
            "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
            "\n",
            "- `index_col` is the column name to use as the index, default is None.\n",
            "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
            "\n",
            "Here is an example code snippet for Spark Data:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "\n",
            "# Creating a dictionary\n",
            "data = {\n",
            "    \"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
            "    \"Age_Years\": [20, 15, 10, 7, 25],\n",
            "    \"Price\": [100000, 200000, 300000, 240000, 120000],\n",
            "}\n",
            "\n",
            "# Creating a pandas DataFrame\n",
            "dataframe = pd.DataFrame(data)\n",
            "label = \"Price\"\n",
            "\n",
            "# Convert to pandas-on-spark dataframe\n",
            "psdf = to_pandas_on_spark(dataframe)\n",
            "```\n",
            "\n",
            "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
            "\n",
            "Here is an example of how to use it:\n",
            "\n",
            "```python\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "columns = psdf.columns\n",
            "feature_cols = [col for col in columns if col != label]\n",
            "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "```\n",
            "\n",
            "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
            "\n",
            "### Estimators\n",
            "\n",
            "#### Model List\n",
            "\n",
            "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
            "\n",
            "#### Usage\n",
            "\n",
            "First, prepare your data in the required format as described in the previous section.\n",
            "\n",
            "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
            "\n",
            "Here is an example code snippet using SparkML models in AutoML:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "# prepare your data in pandas-on-spark format as we previously mentioned\n",
            "\n",
            "automl = flaml.AutoML()\n",
            "settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
            "    \"task\": \"regression\",\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=psdf,\n",
            "    label=label,\n",
            "    **settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
            "\n",
            "## Parallel Spark Jobs\n",
            "\n",
            "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
            "\n",
            "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
            "\n",
            "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
            "\n",
            "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
            "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
            "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
            "\n",
            "An example code snippet for using parallel Spark jobs:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "automl_experiment = flaml.AutoML()\n",
            "automl_settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"task\": \"regression\",\n",
            "    \"n_concurrent_trials\": 2,\n",
            "    \"use_spark\": True,\n",
            "    \"force_cancel\": True,  # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=dataframe,\n",
            "    label=label,\n",
            "    **automl_settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
            "# Research\n",
            "\n",
            "For technical details, please check our research publications.\n",
            "\n",
            "- [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021flaml,\n",
            "    title={FLAML: A Fast and Lightweight AutoML Library},\n",
            "    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},\n",
            "    year={2021},\n",
            "    booktitle={MLSys},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021cfo,\n",
            "    title={Frugal Optimization for Cost-related Hyperparameters},\n",
            "    author={Qingyun Wu and Chi Wang and Silu Huang},\n",
            "    year={2021},\n",
            "    booktitle={AAAI},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021blendsearch,\n",
            "    title={Economical Hyperparameter Optimization With Blended Search Strategy},\n",
            "    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},\n",
            "    year={2021},\n",
            "    booktitle={ICLR},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{liuwang2021hpolm,\n",
            "    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},\n",
            "    author={Susan Xueqing Liu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ACL},\n",
            "}\n",
            "```\n",
            "\n",
            "- [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021chacha,\n",
            "    title={ChaCha for Online AutoML},\n",
            "    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},\n",
            "    year={2021},\n",
            "    booktitle={ICML},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wuwang2021fairautoml,\n",
            "    title={Fair AutoML},\n",
            "    author={Qingyun Wu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ArXiv preprint arXiv:2111.06495},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{kayaliwang2022default,\n",
            "    title={Mining Robust Default Configurations for Resource-constrained AutoML},\n",
            "    author={Moe Kayali and Chi Wang},\n",
            "    year={2022},\n",
            "    booktitle={ArXiv preprint arXiv:2202.09927},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{zhang2023targeted,\n",
            "    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},\n",
            "    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},\n",
            "    booktitle={International Conference on Learning Representations},\n",
            "    year={2023},\n",
            "    url={https://openreview.net/forum?id=0Ij9_q567Ma},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2023EcoOptiGen,\n",
            "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
            "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2303.04673},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2023empirical,\n",
            "    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n",
            "    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2306.01337},\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
            "\n",
            "You want to know how to build a time series forecasting model for stock prices using FLAML. \n",
            "\n",
            "Here's a sample code snippet that demonstrates how to use FLAML with Spark for time series forecasting:\n",
            "\n",
            "```\n",
            "python\n",
            "import pandas as pd\n",
            "import flaml\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "# Assuming you have a pandas dataframe with stock price data\n",
            "stock_data = pd.read_csv(\"stock_data.csv\")\n",
            "\n",
            "# Convert the data into a pandas-on-spark dataframe\n",
            "psdf = to_pandas_on_spark(stock_data)\n",
            "\n",
            "# Prepare the feature columns\n",
            "feature_cols = [\"feature1\", \"feature2\", ...]\n",
            "label = \"stock_price\"\n",
            "\n",
            "# Use VectorAssembler to merge feature columns\n",
            "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = assembler.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "\n",
            "# Define the AutoML settings\n",
            "automl_settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"mean_squared_error\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],  # Use LightGBM with Spark\n",
            "    \"task\": \"regression\",\n",
            "}\n",
            "\n",
            "# Create an AutoML instance\n",
            "automl_experiment = flaml.AutoML()\n",
            "\n",
            "# Fit the AutoML model\n",
            "automl_experiment.fit(dataframe=psdf, \n",
            "                       label=label, \n",
            "                       **automl_settings)\n",
            "\n",
            "# Get the best model\n",
            "best_model = automl_experiment.get_best_estimator()\n",
            "\n",
            "# Use the best model to make predictions\n",
            "predictions = best_model.predict(psdf)\n",
            "```\n",
            "\n",
            "This code snippet demonstrates how to prepare your stock price data, convert it into a pandas-on-spark dataframe, and then use FLAML with Spark to perform time series forecasting. The `estimator_list` is set to `[\"lgbm_spark\"]` to use LightGBM with Spark for distributed training.\n",
            "\n",
            "Note that you'll need to replace `\"feature1\", \"feature2\", ...` with your actual feature columns, and `\"stock_price\"` with your actual label column. Also, you may need to adjust the `automl_settings` according to your specific requirements.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[31m\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "assistant.reset()\n",
        "\n",
        "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
        "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
        "code_problem = \"how to build a time series forecasting model for stock price using FLAML?\"\n",
        "chat_result = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=code_problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxgMIyqstkJX"
      },
      "source": [
        "### Example 4\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\n",
        "\n",
        "Problem: Is there a function named `tune_automl` in FLAML?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-GVccw4mtkJX",
        "outputId": "de1f6641-e66b-42dd-a292-6b8aa2ed2fb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n",
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorDB returns doc_ids:  [['7968cf3c', 'bdfbc921']]\n",
            "\u001b[32mAdding content of doc 7968cf3c to context.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model llama3-70b-8192 not found. Using cl100k_base encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mAdding content of doc bdfbc921 to context.\u001b[0m\n",
            "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
            "\n",
            "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
            "context provided by the user.\n",
            "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
            "For code generation, you must obey the following rules:\n",
            "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
            "Rule 2. You must follow the formats below to write your code:\n",
            "```language\n",
            "# your code\n",
            "```\n",
            "\n",
            "User's question is: Is there a function named `tune_automl` in FLAML?\n",
            "\n",
            "Context is: # Research\n",
            "\n",
            "For technical details, please check our research publications.\n",
            "\n",
            "- [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021flaml,\n",
            "    title={FLAML: A Fast and Lightweight AutoML Library},\n",
            "    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},\n",
            "    year={2021},\n",
            "    booktitle={MLSys},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021cfo,\n",
            "    title={Frugal Optimization for Cost-related Hyperparameters},\n",
            "    author={Qingyun Wu and Chi Wang and Silu Huang},\n",
            "    year={2021},\n",
            "    booktitle={AAAI},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2021blendsearch,\n",
            "    title={Economical Hyperparameter Optimization With Blended Search Strategy},\n",
            "    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},\n",
            "    year={2021},\n",
            "    booktitle={ICLR},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{liuwang2021hpolm,\n",
            "    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},\n",
            "    author={Susan Xueqing Liu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ACL},\n",
            "}\n",
            "```\n",
            "\n",
            "- [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2021chacha,\n",
            "    title={ChaCha for Online AutoML},\n",
            "    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},\n",
            "    year={2021},\n",
            "    booktitle={ICML},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wuwang2021fairautoml,\n",
            "    title={Fair AutoML},\n",
            "    author={Qingyun Wu and Chi Wang},\n",
            "    year={2021},\n",
            "    booktitle={ArXiv preprint arXiv:2111.06495},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{kayaliwang2022default,\n",
            "    title={Mining Robust Default Configurations for Resource-constrained AutoML},\n",
            "    author={Moe Kayali and Chi Wang},\n",
            "    year={2022},\n",
            "    booktitle={ArXiv preprint arXiv:2202.09927},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{zhang2023targeted,\n",
            "    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},\n",
            "    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},\n",
            "    booktitle={International Conference on Learning Representations},\n",
            "    year={2023},\n",
            "    url={https://openreview.net/forum?id=0Ij9_q567Ma},\n",
            "}\n",
            "```\n",
            "\n",
            "- [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wang2023EcoOptiGen,\n",
            "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
            "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2303.04673},\n",
            "}\n",
            "```\n",
            "\n",
            "- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\n",
            "\n",
            "```bibtex\n",
            "@inproceedings{wu2023empirical,\n",
            "    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n",
            "    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n",
            "    year={2023},\n",
            "    booktitle={ArXiv preprint arXiv:2306.01337},\n",
            "}\n",
            "```\n",
            "# Integrate - Spark\n",
            "\n",
            "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
            "\n",
            "- Use Spark ML estimators for AutoML.\n",
            "- Use Spark to run training in parallel spark jobs.\n",
            "\n",
            "## Spark ML Estimators\n",
            "\n",
            "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
            "\n",
            "### Data\n",
            "\n",
            "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
            "\n",
            "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
            "\n",
            "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
            "\n",
            "- `index_col` is the column name to use as the index, default is None.\n",
            "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
            "\n",
            "Here is an example code snippet for Spark Data:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from flaml.automl.spark.utils import to_pandas_on_spark\n",
            "\n",
            "# Creating a dictionary\n",
            "data = {\n",
            "    \"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
            "    \"Age_Years\": [20, 15, 10, 7, 25],\n",
            "    \"Price\": [100000, 200000, 300000, 240000, 120000],\n",
            "}\n",
            "\n",
            "# Creating a pandas DataFrame\n",
            "dataframe = pd.DataFrame(data)\n",
            "label = \"Price\"\n",
            "\n",
            "# Convert to pandas-on-spark dataframe\n",
            "psdf = to_pandas_on_spark(dataframe)\n",
            "```\n",
            "\n",
            "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
            "\n",
            "Here is an example of how to use it:\n",
            "\n",
            "```python\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "columns = psdf.columns\n",
            "feature_cols = [col for col in columns if col != label]\n",
            "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
            "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
            "```\n",
            "\n",
            "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
            "\n",
            "### Estimators\n",
            "\n",
            "#### Model List\n",
            "\n",
            "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
            "\n",
            "#### Usage\n",
            "\n",
            "First, prepare your data in the required format as described in the previous section.\n",
            "\n",
            "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
            "\n",
            "Here is an example code snippet using SparkML models in AutoML:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "# prepare your data in pandas-on-spark format as we previously mentioned\n",
            "\n",
            "automl = flaml.AutoML()\n",
            "settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
            "    \"task\": \"regression\",\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=psdf,\n",
            "    label=label,\n",
            "    **settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
            "\n",
            "## Parallel Spark Jobs\n",
            "\n",
            "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
            "\n",
            "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
            "\n",
            "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
            "\n",
            "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
            "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
            "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
            "\n",
            "An example code snippet for using parallel Spark jobs:\n",
            "\n",
            "```python\n",
            "import flaml\n",
            "\n",
            "automl_experiment = flaml.AutoML()\n",
            "automl_settings = {\n",
            "    \"time_budget\": 30,\n",
            "    \"metric\": \"r2\",\n",
            "    \"task\": \"regression\",\n",
            "    \"n_concurrent_trials\": 2,\n",
            "    \"use_spark\": True,\n",
            "    \"force_cancel\": True,  # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
            "}\n",
            "\n",
            "automl.fit(\n",
            "    dataframe=dataframe,\n",
            "    label=label,\n",
            "    **automl_settings,\n",
            ")\n",
            "```\n",
            "\n",
            "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
            "\n",
            "No, there is no function named `tune_automl` in FLAML.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[31m\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "assistant.reset()\n",
        "\n",
        "# set `human_input_mode` to be `ALWAYS`, so the agent will ask for human input at every step.\n",
        "ragproxyagent.human_input_mode = \"ALWAYS\"\n",
        "qa_problem = \"Is there a function named `tune_automl` in FLAML?\"\n",
        "chat_result = ragproxyagent.initiate_chat(\n",
        "    assistant, message=ragproxyagent.message_generator, problem=qa_problem\n",
        ")  # type \"exit\" to exit the conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-xcOTX0tkJX"
      },
      "source": [
        "### Example 5\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to answer questions for [NaturalQuestion](https://ai.google.com/research/NaturalQuestions) dataset.\n",
        "\n",
        "First, we will create a new document collection which includes all the contextual corpus. Then, we will choose some questions and utilize RetrieveChat to answer them. For this particular example, we will be using the `gpt-3.5-turbo` model, and we will demonstrate RetrieveChat's feature of automatically updating context in case the documents retrieved do not contain sufficient information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hJG-6uJQtkJX"
      },
      "outputs": [],
      "source": [
        "#config_list[0][\"model\"] = \"gpt-35-turbo\"  # change model to gpt-35-turbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qmY2q43ctkJY"
      },
      "outputs": [],
      "source": [
        "corpus_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/corpus.txt\"\n",
        "\n",
        "# Create a new collection for NaturalQuestions dataset\n",
        "# `task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
        "ragproxyagent = RetrieveUserProxyAgent(\n",
        "    name=\"ragproxyagent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    retrieve_config={\n",
        "        \"task\": \"qa\",\n",
        "        \"docs_path\": corpus_file,\n",
        "        \"chunk_token_size\": 2000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
        "        \"collection_name\": \"natural-questions\",\n",
        "        \"chunk_mode\": \"one_line\",\n",
        "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3L4LZVKZtkJY",
        "outputId": "289e4739-f8ea-4e5d-deba-17e33e691ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what is non controlling interest on balance sheet', 'how many episodes are in chicago fire season 4', 'what are bulls used for on a farm', 'has been honoured with the wisden leading cricketer in the world award for 2016', 'who carried the usa flag in opening ceremony']\n",
            "[[\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"], ['23'], ['breeding', 'as work oxen', 'slaughtered for meat'], ['Virat Kohli'], ['Erin Hamlin']]\n"
          ]
        }
      ],
      "source": [
        "# queries_file = \"https://huggingface.co/datasets/thinkall/NaturalQuestionsQA/resolve/main/queries.jsonl\"\n",
        "queries = \"\"\"{\"_id\": \"ce2342e1feb4e119cb273c05356b33309d38fa132a1cbeac2368a337e38419b8\", \"text\": \"what is non controlling interest on balance sheet\", \"metadata\": {\"answer\": [\"the portion of a subsidiary corporation 's stock that is not owned by the parent corporation\"]}}\n",
        "{\"_id\": \"3a10ff0e520530c0aa33b2c7e8d989d78a8cd5d699201fc4b13d3845010994ee\", \"text\": \"how many episodes are in chicago fire season 4\", \"metadata\": {\"answer\": [\"23\"]}}\n",
        "{\"_id\": \"fcdb6b11969d5d3b900806f52e3d435e615c333405a1ff8247183e8db6246040\", \"text\": \"what are bulls used for on a farm\", \"metadata\": {\"answer\": [\"breeding\", \"as work oxen\", \"slaughtered for meat\"]}}\n",
        "{\"_id\": \"26c3b53ec44533bbdeeccffa32e094cfea0cc2a78c9f6a6c7a008ada1ad0792e\", \"text\": \"has been honoured with the wisden leading cricketer in the world award for 2016\", \"metadata\": {\"answer\": [\"Virat Kohli\"]}}\n",
        "{\"_id\": \"0868d0964c719a52cbcfb116971b0152123dad908ac4e0a01bc138f16a907ab3\", \"text\": \"who carried the usa flag in opening ceremony\", \"metadata\": {\"answer\": [\"Erin Hamlin\"]}}\n",
        "\"\"\"\n",
        "queries = [json.loads(line) for line in queries.split(\"\\n\") if line]\n",
        "questions = [q[\"text\"] for q in queries]\n",
        "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
        "print(questions)\n",
        "print(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aBKmU_8ltkJY",
        "outputId": "9d3c14be-76a5-47ba-dfe3-aa8aa245bd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>>>>>>>>>>>  Below are outputs of Case 1  <<<<<<<<<<<<\n",
            "\n",
            "\n",
            "Trying to create collection.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Collection natural-questions already exists.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#os.system(\"rm -rf /tmp/chromadb\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m qa_problem \u001b[38;5;241m=\u001b[39m questions[i]\n\u001b[0;32m----> 7\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_problem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1099\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Callable):\n\u001b[0;32m-> 1099\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:674\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.message_generator\u001b[0;34m(sender, recipient, context)\u001b[0m\n\u001b[1;32m    671\u001b[0m n_results \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    672\u001b[0m search_string \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[43msender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m sender\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m=\u001b[39m problem\n\u001b[1;32m    676\u001b[0m sender\u001b[38;5;241m.\u001b[39mn_results \u001b[38;5;241m=\u001b[39m n_results\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:598\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.retrieve_docs\u001b[0;34m(self, problem, n_results, search_string)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to create collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:349\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent._init_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     IS_TO_CHUNK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_db\u001b[38;5;241m.\u001b[39mactive_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_overwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IS_TO_CHUNK:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/vectordb/chromadb.py:113\u001b[0m, in \u001b[0;36mChromaVectorDB.create_collection\u001b[0;34m(self, collection_name, overwrite, get_or_create)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collection\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Collection natural-questions already exists."
          ]
        }
      ],
      "source": [
        "for i in range(len(questions)):\n",
        "    print(f\"\\n\\n>>>>>>>>>>>>  Below are outputs of Case {i+1}  <<<<<<<<<<<<\\n\\n\")\n",
        "    # reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "    assistant.reset()\n",
        "    #os.system(\"rm -rf /tmp/chromadb\")\n",
        "    qa_problem = questions[i]\n",
        "    chat_result = ragproxyagent.initiate_chat(\n",
        "        assistant, message=ragproxyagent.message_generator, problem=qa_problem, n_results=30\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCQaf364tkJY"
      },
      "source": [
        "In this example, questions were directly selected from the dataset. RetrieveChat was able to answer the questions correctly in the first attempt as the retrieved context contained the necessary information in the first two cases. However, in the last three cases, the context with the highest similarity to the question embedding did not contain the required information to answer the question. As a result, the LLM model responded with `UPDATE CONTEXT`. With the unique and innovative ability to update context in RetrieveChat, the agent automatically updated the context and sent it to the LLM model again. After several rounds of this process, the agent was able to generate the correct answer to the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgcT1frCtkJY"
      },
      "source": [
        "### Example 6\n",
        "\n",
        "[Back to top](#table-of-contents)\n",
        "\n",
        "Use RetrieveChat to answer multi-hop questions for [2WikiMultihopQA](https://github.com/Alab-NII/2wikimultihop) dataset with customized prompt and few-shot learning.\n",
        "\n",
        "First, we will create a new document collection which includes all the contextual corpus. Then, we will choose some questions and utilize RetrieveChat to answer them. For this particular example, we will be using the `gpt-3.5-turbo` model, and we will demonstrate RetrieveChat's feature of automatically updating context in case the documents retrieved do not contain sufficient information. Moreover, we'll demonstrate how to use customized prompt and few-shot learning to address tasks that are not pre-defined in RetrieveChat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EDxxTxJTtkJZ"
      },
      "outputs": [],
      "source": [
        "PROMPT_MULTIHOP = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the context provided by the user. You must think step-by-step.\n",
        "First, please learn the following examples of context and question pairs and their corresponding answers.\n",
        "\n",
        "Context:\n",
        "Kurram Garhi: Kurram Garhi is a small village located near the city of Bannu, which is the part of Khyber Pakhtunkhwa province of Pakistan. Its population is approximately 35000.\n",
        "Trojkrsti: Trojkrsti is a village in Municipality of Prilep, Republic of Macedonia.\n",
        "Q: Are both Kurram Garhi and Trojkrsti located in the same country?\n",
        "A: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is: no.\n",
        "\n",
        "\n",
        "Context:\n",
        "Early Side of Later: Early Side of Later is the third studio album by English singer- songwriter Matt Goss. It was released on 21 June 2004 by Concept Music and reached No. 78 on the UK Albums Chart.\n",
        "What's Inside: What's Inside is the fourteenth studio album by British singer- songwriter Joan Armatrading.\n",
        "Q: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\n",
        "A: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two, the album to release earlier is What's Inside. So the answer is: What's Inside.\n",
        "\n",
        "\n",
        "Context:\n",
        "Maria Alexandrovna (Marie of Hesse): Maria Alexandrovna , born Princess Marie of Hesse and by Rhine (8 August 1824  3 June 1880) was Empress of Russia as the first wife of Emperor Alexander II.\n",
        "Grand Duke Alexei Alexandrovich of Russia: Grand Duke Alexei Alexandrovich of Russia,(Russian:  ; 14 January 1850 (2 January O.S.) in St. Petersburg  14 November 1908 in Paris) was the fifth child and the fourth son of Alexander II of Russia and his first wife Maria Alexandrovna (Marie of Hesse).\n",
        "Q: What is the cause of death of Grand Duke Alexei Alexandrovich Of Russia's mother?\n",
        "A: The mother of Grand Duke Alexei Alexandrovich of Russia is Maria Alexandrovna. Maria Alexandrovna died from tuberculosis. So the answer is: tuberculosis.\n",
        "\n",
        "\n",
        "Context:\n",
        "Laughter in Hell: Laughter in Hell is a 1933 American Pre-Code drama film directed by Edward L. Cahn and starring Pat O'Brien. The film's title was typical of the sensationalistic titles of many Pre-Code films.\n",
        "Edward L. Cahn: Edward L. Cahn (February 12, 1899  August 25, 1963) was an American film director.\n",
        "Q: When did the director of film Laughter In Hell die?\n",
        "A: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is: August 25, 1963.\n",
        "\n",
        "Second, please complete the answer by thinking step-by-step.\n",
        "\n",
        "Context:\n",
        "{input_context}\n",
        "Q: {input_question}\n",
        "A:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qEvdrEQhtkJZ"
      },
      "outputs": [],
      "source": [
        "# create the RetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
        "corpus_file = \"https://huggingface.co/datasets/thinkall/2WikiMultihopQA/resolve/main/corpus.txt\"\n",
        "\n",
        "# Create a new collection for NaturalQuestions dataset\n",
        "ragproxyagent = RetrieveUserProxyAgent(\n",
        "    name=\"ragproxyagent\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=3,\n",
        "    retrieve_config={\n",
        "        \"task\": \"qa\",\n",
        "        \"docs_path\": corpus_file,\n",
        "        \"chunk_token_size\": 2000,\n",
        "        \"model\": config_list[0][\"model\"],\n",
        "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
        "        \"collection_name\": \"2wikimultihopqa\",\n",
        "        \"chunk_mode\": \"one_line\",\n",
        "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "        \"customized_prompt\": PROMPT_MULTIHOP,\n",
        "        \"customized_answer_prefix\": \"the answer is\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WP0iO3vxtkJZ",
        "outputId": "7b31a02c-12a8-491e-cb62-71920c5393ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Which film came out first, Blind Shaft or The Mask Of Fu Manchu?', 'Are North Marion High School (Oregon) and Seoul High School both located in the same country?']\n",
            "[['The Mask Of Fu Manchu'], ['no']]\n"
          ]
        }
      ],
      "source": [
        "# queries_file = \"https://huggingface.co/datasets/thinkall/2WikiMultihopQA/resolve/main/queries.jsonl\"\n",
        "queries = \"\"\"{\"_id\": \"61a46987092f11ebbdaeac1f6bf848b6\", \"text\": \"Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\", \"metadata\": {\"answer\": [\"The Mask Of Fu Manchu\"]}}\n",
        "{\"_id\": \"a7b9672009c311ebbdb0ac1f6bf848b6\", \"text\": \"Are North Marion High School (Oregon) and Seoul High School both located in the same country?\", \"metadata\": {\"answer\": [\"no\"]}}\n",
        "\"\"\"\n",
        "queries = [json.loads(line) for line in queries.split(\"\\n\") if line]\n",
        "questions = [q[\"text\"] for q in queries]\n",
        "answers = [q[\"metadata\"][\"answer\"] for q in queries]\n",
        "print(questions)\n",
        "print(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jI9C_WawtkJa",
        "outputId": "d91b931d-f7ba-429d-be06-ad3eecf14795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>>>>>>>>>>>  Below are outputs of Case 1  <<<<<<<<<<<<\n",
            "\n",
            "\n",
            "Trying to create collection.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tClyde Thompson: Clyde Thompson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Thompson: Clyde Thompson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpson: Clyde Thompson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most note ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Clyde Thompson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most noted for ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te Thompson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most noted for bein ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tmpson( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most noted for being cit ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t( 1910  July 1, 1979) was an American prisoner turned chaplain. He is most noted for being cited an ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t0  July 1, 1979) was an American prisoner turned chaplain. He is most noted for being cited and lab ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tuly 1, 1979) was an American prisoner turned chaplain. He is most noted for being cited and labeled  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t, 1979) was an American prisoner turned chaplain. He is most noted for being cited and labeled as Th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t9) was an American prisoner turned chaplain. He is most noted for being cited and labeled as The Mea ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts an American prisoner turned chaplain. He is most noted for being cited and labeled as The Meanest  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tAmerican prisoner turned chaplain. He is most noted for being cited and labeled as The Meanest Man i ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcan prisoner turned chaplain. He is most noted for being cited and labeled as The Meanest Man in Tex ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trisoner turned chaplain. He is most noted for being cited and labeled as The Meanest Man in Texas. T ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ter turned chaplain. He is most noted for being cited and labeled as The Meanest Man in Texas. The fi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trned chaplain. He is most noted for being cited and labeled as The Meanest Man in Texas. The film ti ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tchaplain. He is most noted for being cited and labeled as The Meanest Man in Texas. The film titled\" ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tain. He is most noted for being cited and labeled as The Meanest Man in Texas. The film titled\" The  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tHe is most noted for being cited and labeled as The Meanest Man in Texas. The film titled\" The Meane ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t most noted for being cited and labeled as The Meanest Man in Texas. The film titled\" The Meanest Ma ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t noted for being cited and labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td for being cited and labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t being cited and labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tg cited and labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted and labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been film ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td labeled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been filmed an ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\teled as The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tas The Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is curre ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te Meanest Man in Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is currently  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnest Man in Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is currently in th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tMan in Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is currently in the pos ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn Texas. The film titled\" The Meanest Man in Texas\" has been filmed and is currently in the post pro ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tas. The film titled\" The Meanest Man in Texas\" has been filmed and is currently in the post producti ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\the film titled\" The Meanest Man in Texas\" has been filmed and is currently in the post production pr ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlm titled\" The Meanest Man in Texas\" has been filmed and is currently in the post production process ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttled\" The Meanest Man in Texas\" has been filmed and is currently in the post production process and  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t The Meanest Man in Texas\" has been filmed and is currently in the post production process and is ba ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tMeanest Man in Texas\" has been filmed and is currently in the post production process and is based o ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tst Man in Texas\" has been filmed and is currently in the post production process and is based on the ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn in Texas\" has been filmed and is currently in the post production process and is based on the true ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tTexas\" has been filmed and is currently in the post production process and is based on the true stor ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t\" has been filmed and is currently in the post production process and is based on the true story and ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t been filmed and is currently in the post production process and is based on the true story and book ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t filmed and is currently in the post production process and is based on the true story and book of t ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted and is currently in the post production process and is based on the true story and book of the sa ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td is currently in the post production process and is based on the true story and book of the same ti ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcurrently in the post production process and is based on the true story and book of the same title,  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tntly in the post production process and is based on the true story and book of the same title, writt ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tin the post production process and is based on the true story and book of the same title, written by ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te post production process and is based on the true story and book of the same title, written by Don  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt production process and is based on the true story and book of the same title, written by Don Umphr ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tduction process and is based on the true story and book of the same title, written by Don Umphrey. I ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ton process and is based on the true story and book of the same title, written by Don Umphrey. It was ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tocess and is based on the true story and book of the same title, written by Don Umphrey. It was prod ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t and is based on the true story and book of the same title, written by Don Umphrey. It was produced  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tis based on the true story and book of the same title, written by Don Umphrey. It was produced by Br ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsed on the true story and book of the same title, written by Don Umphrey. It was produced by Brad Wi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn the true story and book of the same title, written by Don Umphrey. It was produced by Brad Wilson  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t true story and book of the same title, written by Don Umphrey. It was produced by Brad Wilson and C ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t story and book of the same title, written by Don Umphrey. It was produced by Brad Wilson and Casey  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty and book of the same title, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t book of the same title, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t of the same title, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and direc ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\the same title, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed b ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tme title, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed by Jus ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttle, written by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed by Justin W ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\twritten by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed by Justin Ward.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ten by Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thric ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Don Umphrey. It was produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tUmphrey. It was produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convict ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tey. It was produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted mu ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt was produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted murdere ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t produced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Cly ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tuced by Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Ve ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tby Brad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tad Wilson and Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thomp ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlson and Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson(  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tand Casey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tasey Bond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tBond, and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t and directed by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was call ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tdirected by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called   ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tted by Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The M ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty Justin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanes ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttin Ward. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tard. Thrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in T ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThrice- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te- convicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnvicted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted murderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas pris ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trderer Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison of ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tr Clyde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison officia ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tde Vernon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison officials in ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trnon Thompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison officials in 1938 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThompson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison officials in 1938. He  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tson( 1910- 1979) was called  The Meanest Man in Texas  by Texas prison officials in 1938. He was p ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t1910- 1979) was called  The Meanest Man in Texas  by Texas prison officials in 1938. He was placed ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t 1979) was called  The Meanest Man in Texas  by Texas prison officials in 1938. He was placed in a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t) was called  The Meanest Man in Texas  by Texas prison officials in 1938. He was placed in a spec ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t called  The Meanest Man in Texas  by Texas prison officials in 1938. He was placed in a special s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted  The Meanest Man in Texas  by Texas prison officials in 1938. He was placed in a special solita ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tThe Meanest Man in Texas  by Texas prison officials in 1938. He was placed in a special solitary co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\teanest Man in Texas  by Texas prison officials in 1938. He was placed in a special solitary confine ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt Man in Texas  by Texas prison officials in 1938. He was placed in a special solitary confinement  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t in Texas  by Texas prison officials in 1938. He was placed in a special solitary confinement cell  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\texas  by Texas prison officials in 1938. He was placed in a special solitary confinement cell forme ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t by Texas prison officials in 1938. He was placed in a special solitary confinement cell formerly u ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tAustralian Historical Monographs: The Australian Historical Monographs are a series of Historical st ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\talian Historical Monographs: The Australian Historical Monographs are a series of Historical studies ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Historical Monographs: The Australian Historical Monographs are a series of Historical studies priv ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\torical Monographs: The Australian Historical Monographs are a series of Historical studies privately ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tl Monographs: The Australian Historical Monographs are a series of Historical studies privately prin ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tographs: The Australian Historical Monographs are a series of Historical studies privately printed b ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ths: The Australian Historical Monographs are a series of Historical studies privately printed by Geo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\the Australian Historical Monographs are a series of Historical studies privately printed by George M ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tstralian Historical Monographs are a series of Historical studies privately printed by George Mackan ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tian Historical Monographs are a series of Historical studies privately printed by George Mackaness.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tistorical Monographs are a series of Historical studies privately printed by George Mackaness. A fac ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tical Monographs are a series of Historical studies privately printed by George Mackaness. A facsimil ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tMonographs are a series of Historical studies privately printed by George Mackaness. A facsimile rep ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\traphs are a series of Historical studies privately printed by George Mackaness. A facsimile reprint  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t are a series of Historical studies privately printed by George Mackaness. A facsimile reprint of th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ta series of Historical studies privately printed by George Mackaness. A facsimile reprint of the mon ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ties of Historical studies privately printed by George Mackaness. A facsimile reprint of the monograp ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tf Historical studies privately printed by George Mackaness. A facsimile reprint of the monographs wa ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttorical studies privately printed by George Mackaness. A facsimile reprint of the monographs was pub ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tal studies privately printed by George Mackaness. A facsimile reprint of the monographs was publishe ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tudies privately printed by George Mackaness. A facsimile reprint of the monographs was published in  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t privately printed by George Mackaness. A facsimile reprint of the monographs was published in 44 vo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tately printed by George Mackaness. A facsimile reprint of the monographs was published in 44 volumes ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t printed by George Mackaness. A facsimile reprint of the monographs was published in 44 volumes by R ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tted by George Mackaness. A facsimile reprint of the monographs was published in 44 volumes by Review ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty George Mackaness. A facsimile reprint of the monographs was published in 44 volumes by Review Publ ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trge Mackaness. A facsimile reprint of the monographs was published in 44 volumes by Review Publicati ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tackaness. A facsimile reprint of the monographs was published in 44 volumes by Review Publications,  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tess. A facsimile reprint of the monographs was published in 44 volumes by Review Publications, Dubbo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tA facsimile reprint of the monographs was published in 44 volumes by Review Publications, Dubbo, in  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsimile reprint of the monographs was published in 44 volumes by Review Publications, Dubbo, in 1976. ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te reprint of the monographs was published in 44 volumes by Review Publications, Dubbo, in 1976. The  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trint of the monographs was published in 44 volumes by Review Publications, Dubbo, in 1976. The compl ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tof the monographs was published in 44 volumes by Review Publications, Dubbo, in 1976. The complete l ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te monographs was published in 44 volumes by Review Publications, Dubbo, in 1976. The complete list i ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tographs was published in 44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ths was published in 44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No. I.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts published in 44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Ro ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlished in 44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td in 44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t44 volumes by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stev ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlumes by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t by Review Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\teview Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Asso ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Publications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associati ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tications, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associations w ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tons, Dubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associations with A ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tDubbo, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associations with Austra ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t, in 1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associations with Australia.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t1976. The complete list is: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t The complete list is: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcomplete list is: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Lim ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tete list is: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tist is: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited to th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts: No. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. I. -- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty copie ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t-- Robert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty copies. No ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tbert Louis Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tLouis Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- S ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Stevenson: His Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some F ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tenson: His Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictit ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t: His Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Associations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyag ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tciations with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tons with Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Aust ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tith Australia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tustralia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 A ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlia. 9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t9 May 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t 1935. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Lim ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. Limited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tited to thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tto thirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tirty copies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tcopies. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts. No. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. N ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. II. -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. No. II ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t -- Some Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. No. III.   ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tome Fictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. No. III.  Georg ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tictitious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. No. III.  George Aug ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tious Voyages to Australia. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tVoyages to Australia. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus Robi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tes to Australia. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Australia. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson s Jo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tralia. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson s Journey ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. 9 August 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson s Journey to S ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tugust 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson s Journey to South- ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t 1937. Limited to thirty- five copies. No. III.  George Augustus Robinson s Journey to South- East ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. Limited to thirty- five copies. No. III.  George Augustus Robinson s Journey to South- Eastern A ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tited to thirty- five copies. No. III.  George Augustus Robinson s Journey to South- Eastern Austra ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tto thirty- five copies. No. III.  George Augustus Robinson s Journey to South- Eastern Australia,  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tirty- five copies. No. III.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t five copies. No. III.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, with ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t copies. No. III.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, with Geor ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tes. No. III.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, with George He ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\to. III.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, with George Henry H ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tI.  George Augustus Robinson s Journey to South- Eastern Australia, 1884, with George Henry Haydon ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tGeorge Augustus Robinson s Journey to South- Eastern Australia, 1884, with George Henry Haydon s N ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te Augustus Robinson s Journey to South- Eastern Australia, 1884, with George Henry Haydon s Narrat ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tustus Robinson s Journey to South- Eastern Australia, 1884, with George Henry Haydon s Narrative o ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Robinson s Journey to South- Eastern Australia, 1884, with George Henry Haydon s Narrative of Par ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnson s Journey to South- Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts Journey to South- Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of the S ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\turney to South- Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of the Same J ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t to South- Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of the Same Journe ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\touth- Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Eastern Australia, 1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 Nove ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tern Australia, 1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 November  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tustralia, 1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tlia, 1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t1884, with George Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limited t ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t with George Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limited to fif ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t George Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limited to fifty co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tge Henry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limited to fifty copies  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnry Haydon s Narrative of Part of the Same Journey. 15 November 1941. Limited to fifty copies for s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\taydon s Narrative of Part of the Same Journey. 15 November 1941. Limited to fifty copies for sale a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t s Narrative of Part of the Same Journey. 15 November 1941. Limited to fifty copies for sale and te ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tarrative of Part of the Same Journey. 15 November 1941. Limited to fifty copies for sale and ten for ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tive of Part of the Same Journey. 15 November 1941. Limited to fifty copies for sale and ten for pres ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tf Part of the Same Journey. 15 November 1941. Limited to fifty copies for sale and ten for presentat ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt of the Same Journey. 15 November 1941. Limited to fifty copies for sale and ten for presentation.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tthe Same Journey. 15 November 1941. Limited to fifty copies for sale and ten for presentation. No. I ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tame Journey. 15 November 1941. Limited to fifty copies for sale and ten for presentation. No. IV.   ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tourney. 15 November 1941. Limited to fifty copies for sale and ten for presentation. No. IV.  Some  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty. 15 November 1941. Limited to fifty copies for sale and ten for presentation. No. IV.  Some Priva ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t November 1941. Limited to fifty copies for sale and ten for presentation. No. IV.  Some Private Co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tmber 1941. Limited to fifty copies for sale and ten for presentation. No. IV.  Some Private Corresp ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t1941. Limited to fifty copies for sale and ten for presentation. No. IV.  Some Private Corresponden ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Limited to fifty copies for sale and ten for presentation. No. IV.  Some Private Correspondence of ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tted to fifty copies for sale and ten for presentation. No. IV.  Some Private Correspondence of the  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\to fifty copies for sale and ten for presentation. No. IV.  Some Private Correspondence of the Rev.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tty copies for sale and ten for presentation. No. IV.  Some Private Correspondence of the Rev. Samue ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpies for sale and ten for presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Mar ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tfor sale and ten for presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tale and ten for presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and F ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnd ten for presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn for presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t presentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29 Febru ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tentation. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29 February 1 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tion. No. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tNo. IV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limit ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tV.  Some Private Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tSome Private Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to nine ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tPrivate Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tte Correspondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trrespondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies for s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tondence of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies for sale a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tce of the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies for sale and te ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t the Rev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies for sale and ten for ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tRev. Samuel Marsden and Family. 29 February 1942. Limited to ninety copies for sale and ten for pres ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tSamuel Marsden and Family. 29 February 1942. Limited to ninety copies for sale and ten for presentat ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tl Marsden and Family. 29 February 1942. Limited to ninety copies for sale and ten for presentation.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tsden and Family. 29 February 1942. Limited to ninety copies for sale and ten for presentation. No. V ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tand Family. 29 February 1942. Limited to ninety copies for sale and ten for presentation. No. V.  A ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tamily. 29 February 1942. Limited to ninety copies for sale and ten for presentation. No. V.  Accoun ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t. 29 February 1942. Limited to ninety copies for sale and ten for presentation. No. V.  Account of  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tFebruary 1942. Limited to ninety copies for sale and ten for presentation. No. V.  Account of the D ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tary 1942. Limited to ninety copies for sale and ten for presentation. No. V.  Account of the Duel b ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t942. Limited to ninety copies for sale and ten for presentation. No. V.  Account of the Duel betwee ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tLimited to ninety copies for sale and ten for presentation. No. V.  Account of the Duel between Wil ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted to ninety copies for sale and ten for presentation. No. V.  Account of the Duel between William  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t ninety copies for sale and ten for presentation. No. V.  Account of the Duel between William Bland ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tty copies for sale and ten for presentation. No. V.  Account of the Duel between William Bland and  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpies for sale and ten for presentation. No. V.  Account of the Duel between William Bland and Rober ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tfor sale and ten for presentation. No. V.  Account of the Duel between William Bland and Robert Cas ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tale and ten for presentation. No. V.  Account of the Duel between William Bland and Robert Case, wi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnd ten for presentation. No. V.  Account of the Duel between William Bland and Robert Case, with a  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn for presentation. No. V.  Account of the Duel between William Bland and Robert Case, with a Repor ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t presentation. No. V.  Account of the Duel between William Bland and Robert Case, with a Report of  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tentation. No. V.  Account of the Duel between William Bland and Robert Case, with a Report of the T ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tion. No. V.  Account of the Duel between William Bland and Robert Case, with a Report of the Trial, ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tNo. V.  Account of the Duel between William Bland and Robert Case, with a Report of the Trial, Tex  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t.  Account of the Duel between William Bland and Robert Case, with a Report of the Trial, Tex v. Bl ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tccount of the Duel between William Bland and Robert Case, with a Report of the Trial, Tex v. Bland.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt of the Duel between William Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 Se ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tthe Duel between William Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 Septemb ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tuel between William Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 19 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tetween William Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. L ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn William Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limite ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tliam Bland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tBland and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninet ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t and Robert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety cop ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tRobert Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies f ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt Case, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sa ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te, with a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale an ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tth a Report of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tReport of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten for  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt of the Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten for prese ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tthe Trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten for presentati ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\trial, Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten for presentation. N ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Tex v. Bland. 20 September 1942. Limited to ninety copies for sale and ten for presentation. No. VI ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tv. Bland. 20 September 1942. Limited to ninety copies for sale and ten for presentation. No. VI.  S ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tand. 20 September 1942. Limited to ninety copies for sale and ten for presentation. No. VI.  Some P ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t20 September 1942. Limited to ninety copies for sale and ten for presentation. No. VI.  Some Propos ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tptember 1942. Limited to ninety copies for sale and ten for presentation. No. VI.  Some Proposals f ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ter 1942. Limited to ninety copies for sale and ten for presentation. No. VI.  Some Proposals for Es ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t42. Limited to ninety copies for sale and ten for presentation. No. VI.  Some Proposals for Establi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\timited to ninety copies for sale and ten for presentation. No. VI.  Some Proposals for Establishing ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td to ninety copies for sale and ten for presentation. No. VI.  Some Proposals for Establishing Colo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tninety copies for sale and ten for presentation. No. VI.  Some Proposals for Establishing Colonies  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty copies for sale and ten for presentation. No. VI.  Some Proposals for Establishing Colonies in th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ties for sale and ten for presentation. No. VI.  Some Proposals for Establishing Colonies in the Sou ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tor sale and ten for presentation. No. VI.  Some Proposals for Establishing Colonies in the South Se ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tle and ten for presentation. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 1 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td ten for presentation. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 Mar ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t for presentation. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 March 19 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpresentation. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 March 1943. L ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tntation. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 March 1943. Limite ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ton. No. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 March 1943. Limited to  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\to. VI.  Some Proposals for Establishing Colonies in the South Seas. 12 March 1943. Limited to ninet ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t.  Some Proposals for Establishing Colonies in the South Seas. 12 March 1943. Limited to ninety cop ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tome Proposals for Establishing Colonies in the South Seas. 12 March 1943. Limited to ninety copies f ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\troposals for Establishing Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sa ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tals for Establishing Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sale an ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tor Establishing Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sale and ten ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ttablishing Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sale and ten for  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tshing Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sale and ten for prese ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Colonies in the South Seas. 12 March 1943. Limited to ninety copies for sale and ten for presentati ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnies in the South Seas. 12 March 1943. Limited to ninety copies for sale and ten for presentation. N ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tin the South Seas. 12 March 1943. Limited to ninety copies for sale and ten for presentation. No. VI ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te South Seas. 12 March 1943. Limited to ninety copies for sale and ten for presentation. No. VII.   ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tth Seas. 12 March 1943. Limited to ninety copies for sale and ten for presentation. No. VII.  Alexa ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tas. 12 March 1943. Limited to ninety copies for sale and ten for presentation. No. VII.  Alexander  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t2 March 1943. Limited to ninety copies for sale and ten for presentation. No. VII.  Alexander Dalry ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tch 1943. Limited to ninety copies for sale and ten for presentation. No. VII.  Alexander Dalrymple  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t43. Limited to ninety copies for sale and ten for presentation. No. VII.  Alexander Dalrymple s   ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\timited to ninety copies for sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Ser ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td to ninety copies for sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tninety copies for sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admon ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ty copies for sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ties for sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to t ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tor sale and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Pu ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tle and ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td ten for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on th ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t for presentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on the Int ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpresentation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on the Intended ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tntation. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on the Intended Thie ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ton. No. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on the Intended Thief Col ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\to. VII.  Alexander Dalrymple s  A Serious Admonition to the Public on the Intended Thief Colony a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tI.  Alexander Dalrymple s  A Serious Admonition to the Public on the Intended Thief Colony at Bot ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tAlexander Dalrymple s  A Serious Admonition to the Public on the Intended Thief Colony at Botany B ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnder Dalrymple s  A Serious Admonition to the Public on the Intended Thief Colony at Botany Bay.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tDalrymple s  A Serious Admonition to the Public on the Intended Thief Colony at Botany Bay.  With ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tmple s  A Serious Admonition to the Public on the Intended Thief Colony at Botany Bay.  With a Me ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts  A Serious Admonition to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tA Serious Admonition to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 J ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tious Admonition to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tAdmonition to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tition to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limit ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t to the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\the Public on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to nine ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tblic on the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ton the Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\te Intended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tended Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Thief Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and te ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tf Colony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tony at Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for pres ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tt Botany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for presentat ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tany Bay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for presentation.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tay.  With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for presentation. No. V ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t With a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for presentation. No. VIII.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t a Memoir. 12 July 1943. Limited to ninety copied for sale and ten for presentation. No. VIII. Capt ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tmoir. 12 July 1943. Limited to ninety copied for sale and ten for presentation. No. VIII. Captain W ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t 12 July 1943. Limited to ninety copied for sale and ten for presentation. No. VIII. Captain Willia ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tuly 1943. Limited to ninety copied for sale and ten for presentation. No. VIII. Captain William Bli ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t943. Limited to ninety copied for sale and ten for presentation. No. VIII. Captain William Bligh s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tLimited to ninety copied for sale and ten for presentation. No. VIII. Captain William Bligh s Disc ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ted to ninety copied for sale and ten for presentation. No. VIII. Captain William Bligh s Discoveri ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t ninety copied for sale and ten for presentation. No. VIII. Captain William Bligh s Discoveries an ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tty copied for sale and ten for presentation. No. VIII. Captain William Bligh s Discoveries and Obs ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tpied for sale and ten for presentation. No. VIII. Captain William Bligh s Discoveries and Observat ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tfor sale and ten for presentation. No. VIII. Captain William Bligh s Discoveries and Observations  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tale and ten for presentation. No. VIII. Captain William Bligh s Discoveries and Observations in Va ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tnd ten for presentation. No. VIII. Captain William Bligh s Discoveries and Observations in Van Die ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn for presentation. No. VIII. Captain William Bligh s Discoveries and Observations in Van Diemen  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t presentation. No. VIII. Captain William Bligh s Discoveries and Observations in Van Diemen s Lan ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tentation. No. VIII. Captain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tion. No. VIII. Captain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 Octo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tNo. VIII. Captain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1 ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tIII. Captain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1943.  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tCaptain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limit ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tain William Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limited to ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tilliam Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limited to nine ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tm Bligh s Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limited to ninety co ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tgh s Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limited to ninety copies  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t Discoveries and Observations in Van Diemen s Land. 14 October 1943. Limited to ninety copies for s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\toveries and Observations in Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale a ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tes and Observations in Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fi ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td Observations in Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tervations in Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen for  ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tions in Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen for prese ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tin Van Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen for presentati ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tn Diemen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen for presentation. N ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tmen s Land. 14 October 1943. Limited to ninety copies for sale and fifteen for presentation. No. IX ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\ts Land. 14 October 1943. Limited to ninety copies for sale and fifteen for presentation. No. IX.  D ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\td. 14 October 1943. Limited to ninety copies for sale and fifteen for presentation. No. IX.  Duchar ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t October 1943. Limited to ninety copies for sale and fifteen for presentation. No. IX.  Ducharme s ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\tber 1943. Limited to ninety copies for sale and fifteen for presentation. No. IX.  Ducharme s  Jo ...\n",
            "max_tokens is too small to fit a single line of text. Breaking this line:\n",
            "\t943. Limited to ninety copies for sale and fifteen for presentation. No. IX.  Ducharme s  Journal ...\n",
            "2024-09-23 20:44:29,173 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 57508 chunks.\u001b[0m\n",
            "2024-09-23 20:44:58,599 - autogen.agentchat.contrib.vectordb.chromadb - INFO - No content embedding is provided. Will use the VectorDB's embedding function to generate the content embedding.\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m assistant\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      7\u001b[0m qa_problem \u001b[38;5;241m=\u001b[39m questions[i]\n\u001b[0;32m----> 8\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_problem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1099\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Callable):\n\u001b[0;32m-> 1099\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:674\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.message_generator\u001b[0;34m(sender, recipient, context)\u001b[0m\n\u001b[1;32m    671\u001b[0m n_results \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    672\u001b[0m search_string \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[43msender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m sender\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m=\u001b[39m problem\n\u001b[1;32m    676\u001b[0m sender\u001b[38;5;241m.\u001b[39mn_results \u001b[38;5;241m=\u001b[39m n_results\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:598\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.retrieve_docs\u001b[0;34m(self, problem, n_results, search_string)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to create collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:392\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent._init_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m     chunk_ids_set_idx \u001b[38;5;241m=\u001b[39m [chunk_ids\u001b[38;5;241m.\u001b[39mindex(hash_value) \u001b[38;5;28;01mfor\u001b[39;00m hash_value \u001b[38;5;129;01min\u001b[39;00m chunk_ids_set]\n\u001b[1;32m    386\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    387\u001b[0m         Document(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mchunk_ids[idx], content\u001b[38;5;241m=\u001b[39mchunks[idx], metadata\u001b[38;5;241m=\u001b[39msources[idx])\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m chunk_ids_set_idx\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_ids[idx] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_docs_ids\n\u001b[1;32m    390\u001b[0m     ]\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/vectordb/chromadb.py:204\u001b[0m, in \u001b[0;36mChromaVectorDB.insert_docs\u001b[0;34m(self, docs, collection_name, upsert)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/autogen/agentchat/contrib/vectordb/chromadb.py:167\u001b[0m, in \u001b[0;36mChromaVectorDB._batch_insert\u001b[0;34m(self, collection, embeddings, ids, metadatas, documents, upsert)\u001b[0m\n\u001b[1;32m    160\u001b[0m collection_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: documents[i:end_idx],\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m: ids[i:end_idx],\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadatas[i:end_idx] \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: embeddings[i:end_idx] \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    165\u001b[0m }\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m upsert:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcollection_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     collection\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcollection_kwargs)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/chromadb/api/models/Collection.py:483\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/chromadb/api/models/Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/chromadb/api/types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/chromadb/utils/embedding_functions.py:79\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddingFunction.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Documents) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m     78\u001b[0m         Embeddings,\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     84\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:601\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    603\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:668\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    667\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib64/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/Agents-4yRd6dJo/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    437\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(len(questions)):\n",
        "    print(f\"\\n\\n>>>>>>>>>>>>  Below are outputs of Case {i+1}  <<<<<<<<<<<<\\n\\n\")\n",
        "\n",
        "    # reset the assistant. Always reset the assistant before starting a new conversation.\n",
        "    assistant.reset()\n",
        "\n",
        "    qa_problem = questions[i]\n",
        "    chat_result = ragproxyagent.initiate_chat(\n",
        "        assistant, message = ragproxyagent.message_generator, problem=qa_problem, n_results=10,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loop runs forever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "front_matter": {
      "description": "Explore the use of AutoGen's RetrieveChat for tasks like code generation from docstrings, answering complex questions with human feedback, and exploiting features like Update Context, custom prompts, and few-shot learning.",
      "tags": [
        "RAG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "skip_test": "Requires interactive usage"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
